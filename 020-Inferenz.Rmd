

# Inferenz



## Lernsteuerung

```{r chapter-start-sections, echo = FALSE, results = "asis"}
source("funs/chapter-start-sections.R")
chapter_start_sections(title = "Inferenz")
```






```{r echo = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```








## Was ist Inferenz?

```{r include=FALSE}
library(kableExtra)
```




### Deskriptiv- vs. Inferenzstatistik




```{r QM2-Thema1-WasistInferenz-5, echo = FALSE}
knitr::include_graphics("img/desk_vs_inf-crop.png")
```

*Deskriptivstastistik* fasst Stichprobenmerkmale zu Kennzahlen (Statistiken) zusammen.

*Inferenzstatistik* schlie√üt von Statistiken auf Parameter (Kennzahlen von Grundgesamtheiten).

üèã Schlie√üen Sie die Augen und zeichnen Sie obiges Diagramm!


### Wozu ist die Inferenstatistik gut?



:::: {.infobox .quote}
Inferenz bedeutet logisches Schlie√üen; auf Basis von vorliegenden Wissen wird neues Wissen generiert.
:::

:::: {.infobox .quote}
Inferenzstatistik ist ein Verfahren, das mathematische Modelle (oft aus der Stochastik) verwendet, um von einer bestimmten Datenlage, die eine Stichprobe einer Grundgesamtheit darstellt, allgemeine Schl√ºsse zu ziehen.
:::



üèãÔ∏èÔ∏è Heute Nacht vor dem Schlafen wiederholen Sie die Definition. √úben Sie jetzt schon mal.

<!-- ## Die drei Aufgaben der Inferenzstatistik -->


<!-- 1. Von der Stichprobe auf die Grundgesamtheit schlie√üen  -->

<!-- 2. Von der Experimental- auf die Kontrollgruppe zu schlie√üen -->

<!-- 3. Vom beobachteten Messwert auf das zugrundeliegende Konstrukt zu schlie√üen -->


### Deskriptiv- und Inferenzstatistik gehen Hand in Hand


üèãÔ∏è F√ºr jede Statistik (Kennzahl  von Stichprobendaten) kann man die Methoden der Inferenzstatistik verwenden, z.B.:



```{r QM2-Thema1-WasistInferenz-6, echo = FALSE}
x <- tribble(
    ~Kennwert, ~Stichprobe, ~Grundgesamtheit,
  "Mittelwert", "$\\bar{X}$", "$\\mu$",
  "Streuung", "$sd$", "$\\sigma$",
  "Anteil", "$p$", "$\\pi$",
  "Korrelation", "$r$", "$\\rho$" ,
  "Regression", "$b$", "$\\beta$"
  
)

kable(x, format = "latex", escape = FALSE, booktabs = FALSE) %>%
  kable_styling(position = "center")
```


F√ºr Statistiken (Stichprobe) verwendet man lateinische Buchstaben; f√ºr Parameter (Population) verwendet man griechische Buchstaben.

üèãÔ∏è Geben Sie die griechischen Buchstaben f√ºr typische Statistiken an!


### Sch√§tzen von Parametern einer Grundgesamtheit


Meist begn√ºgt man sich nicht mit Aussagen f√ºr eine Stichprobe, sondern will auf eine Grundgesamtheit verallgemeinern.

Leider sind die Parameter einer Grundgesamtheit zumeist unbekannt, daher muss man sich mit *Sch√§tzungen* begn√ºgen.

Sch√§tzwerte werden mit einem "Dach" √ºber dem Kennwert gekennzeichnet, z.B.


```{r QM2-Thema1-WasistInferenz-7, echo = FALSE}
x <- tribble(
    ~Kennwert, ~Stichprobe, ~Grundgesamtheit, ~Sch√§tzwert,
  "Mittelwert", "$\\bar{X}$", "$\\mu$", "$\\hat{\\mu}$",
  "Streuung", "$sd$", "$\\sigma$", "$\\hat{\\sigma}$",
  "Anteil", "$p$", "$\\pi$", "$\\hat{\\pi}$",
  "Korrelation", "$r$", "$\\rho$", "$\\hat{\\rho}$" ,
  "Regression", "$b$", "$\\beta$", "$\\hat{\\beta}$"
  
)

kable(x, format = "latex", escape = FALSE, booktabs = FALSE) %>%
  kable_styling(position = "center")
```



### Beispiele f√ºr  inferenzstatistische Fragestellungen



Sie testen zwei Varianten Ihres Webshops (V1 und V2), die sich im Farbschema unterscheiden und ansonsten identisch sind: Hat das Farbschema einen Einfluss auf den Umsatz?

- Dazu vergleichen Sie den mittleren Umsatz pro Tag von V1 vs. V2, $\bar{X}_{V1}$ und $\bar{X}_{V2}$.

- Die Mittelwerte unterscheiden sich etwas, $\bar{X}_{V1} > \bar{X}_{V2}$

- Sind diese Unterschiede "zuf√§llig" oder "substanziell"? Gilt also $\mu_{V1} > \mu_{V2}$ oder gilt  $\mu_{V1} \le \mu_{V2}$?

- Wie gro√ü ist die Wahrscheinlichkeit^[oft mit *Pr* oder *p* abgek√ºrzt, f√ºr *probability*] $Pr(\mu_{V1} > \mu_{V2})$?


üèãÔ∏è *Predictive Maintenance* ist ein Anwendungsfeld inferenzstatistischer Modellierung. Lesen Sie dazu S. 3 [dieses Berichts](https://www.rolandberger.com/publications/publication_pdf/roland_berger_vdma_predictive_maintenance_d_1.pdf)!


## Unsicherheit

### Inferenz beinhaltet Unsicherheit

Inferenzstatistische Schl√ºsse sind mit Unsicherheit behaftet: 
Schlie√ülich kennt man nur einen Teil (die Stichprobe) eines Ganzen (die Population),
m√∂chte aber vom Teil auf das Ganze schlie√üen.

Zur Bemessung der Unsicherheit bedient man sich der Wahrscheinlichkeitsrechnung (wo immer m√∂glich).



Die Wahrscheinlichkeitstheorie bzw. -rechnung wird auch als die Mathematik des Zufalls bezeichnet.


:::: {.infobox .quote}
Unter einem zuf√§lligen Ereignis (random) verstehen wir ein Ereignis, das nicht (komplett) vorherzusehen ist, wie etwa die Augenzahl Ihres n√§chsten W√ºrfelwurfs. Zuf√§llig bedeutet nicht (zwangsl√§ufig), dass das Ereignisse keine Ursachen besitzt. So gehorchen die Bewegungen eines W√ºrfels den Gesetzen der Physik, nur sind uns diese oder die genauen Randbedingungen nicht (ausreichend) bekannt.
:::



üèã Welche physikalischen Randbedingungen wirken wohl auf einen M√ºnzwurf ein?



### Beispiele zur Quantifizierung von Ungewissheit


Aussagen mit Unsicherheit k√∂nnen unterschiedlich pr√§zise formuliert sein.

- Morgen regnet's $\Leftrightarrow$ Morgen wird es hier mehr als 0 mm Niederschlag geben ($p=97\%$).

- Methode $A$ ist besser als Methode $B$ $\Leftrightarrow$ Mit einer Wahrscheinlichkeit von 57% ist der Mittelwert f√ºr Methode $A$ h√∂her als f√ºr Methode $B$.

- Die Maschine f√§llt demn√§chst aus $\Leftrightarrow$ Mit einer Wahrscheinlichkeit von 97% wird die Maschine in den n√§chsten 1-3 Tagen ausfallen, laut unserem Modell.

- Die Investition lohnt sich $\Leftrightarrow$ Die Investition hat einen Erwartungswert von 42 Euro; mit 90% Wahrscheinlichkeit wird der  Gewinn zwischen -10000 und 100 Euro.



üèã  Geben Sie weitere Beispiele an!





### Visualisierung von Punktsch√§tzungen


```{r QM2-Thema1-WasistInferenz-12, echo = FALSE}
lm1_glm <- lm(mpg ~ hp, data = mtcars)

mtcars <- 
  mtcars %>% 
  mutate(pred = 30 - hp*0.07)


pred_interval <-
  tibble(
    hp = seq(min(mtcars$hp), max(mtcars$hp), by = 1),
    mpg = predict(lm1_glm, newdata = data.frame(hp)),
    lwr = mpg - 2*3,
    upr = mpg + 2*3
  )

```


```{r plot1, fig.asp=0.5, echo = FALSE}
plot1 <- 
  ggplot(mtcars,
       aes(x = hp, y = mpg)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  annotate("point", x = 200, 
           y = predict(lm1_glm, newdata = data.frame(hp = 200)),
           color = "red",
           alpha = .5,
           size = 5)
plot1
```




Rot markiert: Die Punktsch√§tzung von `mpg` f√ºr `hp=200`.

üèã  Geben Sie ein vergleichbares Beispiel an!




### Die Punktsch√§tzung ber√ºcksichtigt nicht die Ungewissheit des Models

Zwei Arten von Ungewissheit m√ºssen wir (mindestens) in unseren Vorhersagen ber√ºcksichtigen:

1. zur Lage der Regressionsgeraden ($\beta_0$, $\beta_1$)
2. zu Einfl√ºssen, die unser Modell nicht kennt ($\epsilon, \sigma$)


#### Unsicherheit in $\beta_0, \beta1$

```{r plt-uncert-beta0beta1, echo = FALSE}
plot2 <- 
  ggplot(mtcars,
       aes(x = hp, y = mpg)) +
  geom_point() +
  geom_smooth(method = "lm", se = TRUE) +
  annotate("point", x = 200, 
           y = predict(lm1_glm, newdata = data.frame(hp = 200)),
           color = "red",
           alpha = .5,
           size = 5)
plot2
```


#### Unsicherheit durch $\epsilon$ ($\sigma$)

```{r plot-uncertainty-eps, echo = FALSE}
ggplot(mtcars) +
  aes(x = hp, y = mpg) +
  geom_point()+
  geom_ribbon(data = pred_interval,
              aes(ymin = lwr, ymax = upr,
                  y = mpg,
                  x = hp),
              fill = "blue",
              alpha = .1) + 
  geom_smooth(method = "lm", se = FALSE) +
  annotate("point", x = 200, 
           y = predict(lm1_glm, newdata = data.frame(hp = 200)),
           color = "red",
           alpha = .5,
           size = 5)


```





### Vorhersage-Intervall: ber√ºcksichtigt Ungewissheit in $\beta_0, \beta_1, \epsilon$

Das Vorhersage-Intervall ber√ºcksichtigt Ungewissheit in $\beta_0, \beta_1, \epsilon$ bei der Vorhersage von $\hat{y_i}$.


```{r plot-pred-interval, echo = FALSE}
pred_interval2 <-
  predict(lm1_glm, 
          newdata = data.frame(hp = pred_interval$hp), 
          interval = "prediction") %>% 
  as_tibble() %>% 
  rename(mpg = fit) %>% 
  mutate(hp = pred_interval$hp)



ggplot(mtcars) +
  aes(x = hp, y = mpg) +
  geom_point()+
  geom_ribbon(data = pred_interval2,
              aes(ymin = lwr, ymax = upr,
                  y = mpg,
                  x = hp),
              fill = "blue",
              alpha = .1) + 
  geom_smooth(method = "lm", se = FALSE) +
  annotate("point", x = 200, 
           y = predict(lm1_glm, newdata = data.frame(hp = 200)),
           color = "red",
           alpha = .5,
           size = 5)
```


üèã Interpretieren Sie den Ungewissheitskorridor!










## Klassische vs. Bayes-Inferenz


### Klassische Inferenz: Frequentismus


- Die Ber√ºcksichtigung von Vorwissen zum Sachgegenstand wird vom Frequentismus als subjektiv zur√ºckgewiesen.
- Nur die Daten selber fliesen in die Ergebnisse ein
- Wahrscheinlichkeit wird √ºber relative H√§ufigkeiten definiert.
- Es ist nicht m√∂glich, die Wahrscheinlichkeit einer Hypothese anzugeben. 
- Stattdessen wird angegeben, wie h√§ufig eine vergleichbare Datenlage zu erwarten ist, wenn die Hypothese gilt und der Versuch sehr h√§ufig wiederholt ist.
- Ein Gro√üteil der Forschung (in den Sozialwissenschaften) verwendet diesen Ansatz.


### Bayesianische Inferenz

- Vorwissen (Priori-Wissen) flie√üt explizit in die Analyse ein (zusammen mit den Daten).
- *Wenn* das Vorwissen gut ist, wird die Vorhersage genauer, ansonsten ungenauer.
- Die Wahl des Vorwissens muss explizit (kritisierbar) sein.
- In der Bayes-Inferenz sind Wahrscheinlichkeitsaussagen f√ºr Hypothesen m√∂glich.
- Die Bayes-Inferenz erfordert mitunter viel Rechenzeit und ist daher erst in den letzten Jahren (f√ºr g√§ngige Computer) komfortabel geworden.


### Vergleich von Wahrscheinlichkeitsaussagen



#### Frequentismus

- zentrale Statistik: *p-Wert*

- "Wie wahrscheinlich ist der Wert der Teststatistik (oder noch extremere Werte), vorausgesetzt die Nullhypothese gilt und man wiederholt den Versuch unendlich oft (mit gleichen Bedingungen, aber zuf√§llig verschieden und auf Basis unseres Modells)?"


#### Bayes-Statistik

- zentrale Statistik: *Posteriori-Verteilung*

- "Wie wahrscheinlich ist die Forschungshypothese, jetzt, nachdem wir die Daten kennen, auf Baiss unseres Modells?"


üèã  Recherchieren Sie eine Definition des p-Werts und lesen Sie sie genau.

### Frequentist und Bayesianer


```{r QM2-Thema1-WasistInferenz-22, out.width="50%", echo = FALSE}
knitr::include_graphics("https://imgs.xkcd.com/comics/frequentists_vs_bayesians_2x.png")
```


[Quelle](https://xkcd.com/1132/)


### Der p-Wert ist wenig intuitiv

<a href="https://imgflip.com/i/6m29tz"><img src="https://i.imgflip.com/6m29tz.jpg" width = "400" title="made at imgflip.com"/></a><div><a href="https://imgflip.com/memegenerator">from Imgflip Meme Generator</a></div>



### Beispiel zum Nutzen von Apriori-Wissen 1




- Ein Betrunkener behauptet, er k√∂nne hellsehen.

- Er wirft eine M√ºnze 10 Mal und sagt jedes Mal korrekt vorher, welche Seite oben landen wird.

- Die Wahrscheinlichkeit dieses Ergebnisses ist sehr gering ($2^{-10}$) unter der Hypothese, dass die M√ºnze fair ist, dass Ergebnis also "zuf√§llig" ist.

- Unser Vorwissen l√§sst uns allerdings trotzdem an der Hellsichtigkeit des Betrunkenen zweifeln, so dass die meisten von uns  die Hypothese von der Zuf√§lligkeit des Ergebnisses wohl nicht verwerfen.



### Beispiel zum Nutzen von Apriori-Wissen 2





- Eine Studie fand einen "gro√üen Effekt" auf das Einkommen von Babies, eine Stunde pro Woche w√§hrend zwei Jahren an einem psychosozialen Entwicklungsprogramm teilnahmen (im Vergleich zu einer Kontrollgruppe), $n=127$.

- Nach 20 Jahren war das mittlere Einkommen der Experimentalgruppe um 42% h√∂her (als in der Kontrollgruppe) mit einem Konfidenzintervall von 
[+2%,+98%].

- Allerdings l√§sst uns unser Vorwissen vermuten, dass so ein Treatment das Einkommen nach 20 Jahren kaum verdoppeln l√§sst. Wir w√ºrden den Effekt lieber in einem konservativeren Intervall sch√§tzen (enger um Null).





<!-- # Wachstum -->

<!-- ## Seerose -->

<!-- - Eine Seerose w√§chst auf einem Teich. [Sch√∂n.](https://www.flickr.com/photos/182338742@N07/49286585198/in/faves-193287163@N02/) -->
<!-- - Tag 1: 1 Seerose. Tag 2: 2 Seerosen. Tag 3: 4 Seerosen, etc. -->
<!-- - Am Tag 100 ist der See komplett mit Seerosen bedeckt. -->

<!-- **An welchem Tag ist der See zu 50% mit Seerosen bedeckt?** -->


<!-- ## Wachstumsschritte der Seerose -->


<!-- $$\text{Menge} = 2^{\text{Tage}}$$ -->

<!-- :::::: {.columns} -->
<!-- ::: {.column width="50%"} -->

<!-- ```{r tab11noeval, results = "hide", eval = TRUE, echo = TRUE} -->
<!-- d <- tibble( -->
<!--   Tag = 0:10, -->
<!--   Menge = 2^Tag)  -->
<!-- ``` -->
<!-- :::  -->
<!-- ::: {.column width="50%"} -->
<!-- ```{r tab11, results = "asis"} -->
<!-- tab11 <- -->
<!--   tibble(Tag = 0:10, -->
<!--          Menge = 2^Tag) %>%  -->
<!--   kable() -->

<!-- print(tab11) -->
<!-- ``` -->
<!-- ::: -->
<!-- :::::: -->






<!-- ## Der Logarithmus gibt die Anzahl der (Wachstums-)Tage -->


<!-- ```{r echo = TRUE, size = "tiny"} -->
<!-- log(d$Menge, base = 2) -->
<!-- ``` -->


<!-- *Logarithmieren* liefert von einer Zahl (hier `Menge`) den Exponenten zu einer Basis (hier `2`) zur√ºck. -->

<!-- Umgekehrt liefert *Potenzieren* zu einer Basis (hier `2`) die `Menge` zur√ºck. -->

<!-- ```{r echo = TRUE, size = "tiny"} -->
<!-- 2^d$Tag -->
<!-- ``` -->


<!-- Wachstumsprozesse sind oft multiplikativ, z.B. eine Seerose, die sich in einem Zeitabschnitt $t$ verdoppelt. -->

<!-- ## Rechenregeln f√ºr Potenzen -->


<!-- - $a^n = a \cdot a \cdot a \ldots a$ ($n$ Faktoren, $n \in \mathbb{N}$) -->
<!-- - $a^1 = a$ -->
<!-- - $a^0 = 1$ -->
<!-- - $a^{-n} = \frac{1}{a^n}$ -->
<!-- - $a^{\frac{1}{n}} = \sqrt[n]{a}$ -->
<!-- - $a^n \cdot a^m = a^{n+m}$ -->
<!-- - $\frac{a^n}{a^m} = a^{n-m}$ -->
<!-- - $a^n \cdot a^m = (a\cdot b)^n$ -->
<!-- - $\frac{a^n}{b^n} = \left(\frac{a}{b}\right)^n$ -->
<!-- - $(a^n)^m = a^{(n\cdot m)}$ -->


<!-- ## Logarithmus -->

<!-- Die Zahl $x \in \mathbb{R}$ mit $b^x=a$ hei√üt Logarithmus von $a$ zur Basis $b$. Sie wird mit $x = log_b(a)$ bezeichnet [@cramer_vorkurs_2015]. Dabei seien $a,b > 0$ mit $b \ne1$. -->


<!-- ```{r echo = TRUE} -->
<!-- log(c(2, 4, 8), base = 2) -->
<!-- log(c(10, 100, 1000), base = 10) -->
<!-- log(c(2.71, 2.71^2)) %>% round() -->
<!-- ``` -->


<!-- G√§ngige Basen sind 2, 10 und $e$ (Eulersche Zahl: $2.7178...$). -->




<!-- ## Rechenregeln zum Logarithmus -->

<!-- - $\text{log}_b(1)=0$ -->
<!-- - $\text{log}_b(b)=1$ -->
<!-- - $b^{\text{log}_b(a)}=a$ -->
<!-- - $\text{log}_b(b^a)=a$ -->

<!-- - $\text{log}_c(a\cdot b) = \text{log}_c(a) + \text{log}_c(b)$ -->
<!-- - $\text{log}_c(\frac{a}{b}) = \text{log}_c(a) - \text{log}_c(b)$ -->
<!-- - $\text{log}_c(b^a) = a \cdot \text{log}_c(b)$ -->







## Literatur
