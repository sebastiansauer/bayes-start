[["index.html", "Bayes:Start Eine EinfÃ¼hrung in die Bayes-Statistik Zu diesem Buch 0.1 tl;dr 0.2 Hinweise 0.3 Kudos", " Bayes:Start Eine EinfÃ¼hrung in die Bayes-Statistik Sebastian Sauer Letzte Aktualisierung: 2022-07-07 21:38:41 Zu diesem Buch Bildquelle: Klara Schaumann, Lizenz: CC-BY 0.1 tl;dr Bayes-Start ist ein frei verfÃ¼gbarer EinfÃ¼hrungskurs in die Bayes-Statistik. Ziel ist, die grundlegende Logik von Bayes-Inferenz zu vermitteln und zu zeigen, wie man mit linearen Modellen viele typische Forschungsfragen beantworten kann. Ein wichtiger Baustein betrifft kausale Ãœberlegungen, also die Frage, wie man kausale Forschungsfragen statistisch umsetzt und beurteilt. Als Software wird R verwendet R Core Team (2019) ; fÃ¼r Bayes-Modellierung wird das R-Paket rstanarm verwendet Goodrich et al. (2020) Inhaltlich wird zum Teil auf Richard McElreathâ€™s hervorragenden Lehrbuch, Statistical Rethinking aufgebaut McElreath (2020). AuÃŸerdem stellt Gelmanâ€™s et al.Â neues, ebenfalls sehr gutes Lehrbuch Regression and other Stories Gelman, Hill, and Vehtari (2021) eine Grundlage fÃ¼r die Inhalte dieses Kurses dar. Beide BÃ¼cher sind als LektÃ¼re zu empfehlen. 0.2 Hinweise Dieser Kurs ist lizensiert unter der MIT Lizenz. Das ist eine permissive Lizenz, die erlaubt, dass Sie diesen Kurs frei verwenden kÃ¶nnen. Sie haben (nur) die Verpflichtung, zu zitieren und auf die Lizenzart hinzuweisen. Installation von R und seinen Freunden Installation der Software fÃ¼r Bayes-Analysen Ãœbergreifende Lernziele Mitarbeit oder VerbesserungsvorschlÃ¤ge: am besten als Github Issue einstellen. Der Quellcode zu den Folien findet sich hier. 0.3 Kudos Beim Schreiben dieses Kurses habe ich auf der Vorarbeit vieler Menschen aufgebaut. Viele Menschen haben mich unterstÃ¼tzt, groÃŸzÃ¼gig und auf verschiedene Weise. Einige mÃ¶chte ich herausgreifen, um Danke zu sagen: Kollegis wie Karsten LÃ¼bke, von denen ich viel gelernt habe. Richard McElreath fÃ¼r sein inspirierendes Lehrbuch. Alle Open-Source-Entwickler, die Projekte wie dieses Ã¼berhaupt erst mÃ¶glichen machen. ğŸ’Œ References "],["hinweise-1.html", "Kapitel 1 Hinweise 1.1 Lernziele 1.2 Voraussetzungen 1.3 Lernhilfen 1.4 Modulzeitplan 1.5 Literatur 1.6 FAQ", " Kapitel 1 Hinweise knitr::opts_chunk$set(cache = FALSE) 1.1 Lernziele Nach diesem Kurs sollten Sie grundlegende Konzepte des statistischen Lernens verstehen und mit R anwenden kÃ¶nnen gÃ¤ngige Prognose-Algorithmen kennen, in GrundzÃ¼gen verstehen und mit R anwenden kÃ¶nnen die GÃ¼te und Grenze von Prognosemodellen einschÃ¤tzen kÃ¶nnen 1.2 Voraussetzungen Um von diesem Kurs am besten zu profitieren, sollten Sie folgendes Wissen mitbringen: grundlegende Kenntnisse im Umgang mit R, mÃ¶glichst auch mit dem tidyverse grundlegende Kenntnisse der deskriptiven Statistik grundlegende Kenntnis der Regressionsanalyse 1.3 Lernhilfen 1.3.1 Software Installieren Sie R und seine Freunde. Installieren Sie die folgende R-Pakete: tidyverse tidymodels weitere Pakete werden im Unterricht bekannt gegeben (es schadet aber nichts, jetzt schon Pakete nach eigenem Ermessen zu installieren) R Syntax aus dem Unterricht findet sich im Github-Repo bzw. Ordner zum jeweiligen Semester. 1.3.2 Videos Playlist zu den Themen Auf dem YouTube-Kanal des Autors finden sich eine Reihe von Videos mit Bezug zum Inhalt dieses Buches. 1.3.3 Online-Zusammenarbeit Hier finden Sie einige Werkzeuge, die das Online-Zusammenarbeiten vereinfachen: Frag-Jetzt-Raum zum anonymen Fragen stellen wÃ¤hrend des Unterrichts. Der Keycode wird Ihnen vom Dozenten bereitgestellt. Padlet zum einfachen (und anonymen) Hochladen von Arbeitsergebnissen der Studentis im Unterricht. Wir nutzen es als eine Art Pinwand zum Sammeln von ArbeitsbeitrÃ¤gen. Die Zugangsdaten stellt Ihnen der Dozent bereit. 1.4 Modulzeitplan 1.5 Literatur jklÃ¶ 1.6 FAQ "],["inferenz.html", "Kapitel 2 Inferenz 2.1 Lernsteuerung 2.2 Was ist Inferenz? 2.3 Unsicherheit 2.4 Klassische vs.Â Bayes-Inferenz 2.5 Literatur", " Kapitel 2 Inferenz 2.1 Lernsteuerung 2.1.1 Vorbereitung Lesen Sie die Hinweise zum Modul. Installieren (oder Updaten) Sie die fÃ¼r dieses Modul angegeben Software. Lesen Sie die angegebene Literatur. 2.1.2 Lernziele die Definition von Inferenzstatistik sowie Beispiele fÃ¼r inferenzstatistische Fragestellungen nennen zentrale Begriffe der Inferenzstatistik nennen und in GrundzÃ¼gen erklÃ¤ren den Nutzen von Inferenzstatistik nennen erlÃ¤utern, in welchem Zusammenhang Ungewissheit zur Inferenzstatistik steht Unterschiede zwischen klassischer und Bayes-Inferenz benennen Vor- und Nachteile der klassischen vs.Â Bayes-Inferenz diskutieren Die grundlegende Herangehensweise zur Berechnung des p-Werts informell erklÃ¤ren kÃ¶nnen 2.1.3 Literatur ROS, Kap. 1 2.1.4 Hinweise Bitte beachten Sie die Hinweise zum PrÃ¤senzunterricht und der Streamingoption. Bitte stellen Sie sicher, dass Sie einen einsatzbereiten Computer haben und dass die angegebene Software (in aktueller Version) lÃ¤uft. 2.2 Was ist Inferenz? 2.2.1 Deskriptiv- vs.Â Inferenzstatistik Deskriptivstastistik fasst Stichprobenmerkmale zu Kennzahlen (Statistiken) zusammen. Inferenzstatistik schlieÃŸt von Statistiken auf Parameter (Kennzahlen von Grundgesamtheiten). ğŸ‹ SchlieÃŸen Sie die Augen und zeichnen Sie obiges Diagramm! 2.2.2 Wozu ist die Inferenstatistik gut? Inferenz bedeutet logisches SchlieÃŸen; auf Basis von vorliegenden Wissen wird neues Wissen generiert. Inferenzstatistik ist ein Verfahren, das mathematische Modelle (oft aus der Stochastik) verwendet, um von einer bestimmten Datenlage, die eine Stichprobe einer Grundgesamtheit darstellt, allgemeine SchlÃ¼sse zu ziehen. ğŸ‹ï¸ï¸ Heute Nacht vor dem Schlafen wiederholen Sie die Definition. Ãœben Sie jetzt schon mal. 2.2.3 Deskriptiv- und Inferenzstatistik gehen Hand in Hand ğŸ‹ï¸ FÃ¼r jede Statistik (Kennzahl von Stichprobendaten) kann man die Methoden der Inferenzstatistik verwenden, z.B.: FÃ¼r Statistiken (Stichprobe) verwendet man lateinische Buchstaben; fÃ¼r Parameter (Population) verwendet man griechische Buchstaben. ğŸ‹ï¸ Geben Sie die griechischen Buchstaben fÃ¼r typische Statistiken an! 2.2.4 SchÃ¤tzen von Parametern einer Grundgesamtheit Meist begnÃ¼gt man sich nicht mit Aussagen fÃ¼r eine Stichprobe, sondern will auf eine Grundgesamtheit verallgemeinern. Leider sind die Parameter einer Grundgesamtheit zumeist unbekannt, daher muss man sich mit SchÃ¤tzungen begnÃ¼gen. SchÃ¤tzwerte werden mit einem â€œDachâ€ Ã¼ber dem Kennwert gekennzeichnet, z.B. 2.2.5 Beispiele fÃ¼r inferenzstatistische Fragestellungen Sie testen zwei Varianten Ihres Webshops (V1 und V2), die sich im Farbschema unterscheiden und ansonsten identisch sind: Hat das Farbschema einen Einfluss auf den Umsatz? Dazu vergleichen Sie den mittleren Umsatz pro Tag von V1 vs.Â V2, \\(\\bar{X}_{V1}\\) und \\(\\bar{X}_{V2}\\). Die Mittelwerte unterscheiden sich etwas, \\(\\bar{X}_{V1} &gt; \\bar{X}_{V2}\\) Sind diese Unterschiede â€œzufÃ¤lligâ€ oder â€œsubstanziellâ€? Gilt also \\(\\mu_{V1} &gt; \\mu_{V2}\\) oder gilt \\(\\mu_{V1} \\le \\mu_{V2}\\)? Wie groÃŸ ist die Wahrscheinlichkeit1 \\(Pr(\\mu_{V1} &gt; \\mu_{V2})\\)? ğŸ‹ï¸ Predictive Maintenance ist ein Anwendungsfeld inferenzstatistischer Modellierung. Lesen Sie dazu S. 3 dieses Berichts! 2.3 Unsicherheit 2.3.1 Inferenz beinhaltet Unsicherheit Inferenzstatistische SchlÃ¼sse sind mit Unsicherheit behaftet: SchlieÃŸlich kennt man nur einen Teil (die Stichprobe) eines Ganzen (die Population), mÃ¶chte aber vom Teil auf das Ganze schlieÃŸen. Zur Bemessung der Unsicherheit bedient man sich der Wahrscheinlichkeitsrechnung (wo immer mÃ¶glich). Die Wahrscheinlichkeitstheorie bzw. -rechnung wird auch als die Mathematik des Zufalls bezeichnet. Unter einem zufÃ¤lligen Ereignis (random) verstehen wir ein Ereignis, das nicht (komplett) vorherzusehen ist, wie etwa die Augenzahl Ihres nÃ¤chsten WÃ¼rfelwurfs. ZufÃ¤llig bedeutet nicht (zwangslÃ¤ufig), dass das Ereignisse keine Ursachen besitzt. So gehorchen die Bewegungen eines WÃ¼rfels den Gesetzen der Physik, nur sind uns diese oder die genauen Randbedingungen nicht (ausreichend) bekannt. ğŸ‹ Welche physikalischen Randbedingungen wirken wohl auf einen MÃ¼nzwurf ein? 2.3.2 Beispiele zur Quantifizierung von Ungewissheit Aussagen mit Unsicherheit kÃ¶nnen unterschiedlich prÃ¤zise formuliert sein. Morgen regnetâ€™s \\(\\Leftrightarrow\\) Morgen wird es hier mehr als 0 mm Niederschlag geben (\\(p=97\\%\\)). Methode \\(A\\) ist besser als Methode \\(B\\) \\(\\Leftrightarrow\\) Mit einer Wahrscheinlichkeit von 57% ist der Mittelwert fÃ¼r Methode \\(A\\) hÃ¶her als fÃ¼r Methode \\(B\\). Die Maschine fÃ¤llt demnÃ¤chst aus \\(\\Leftrightarrow\\) Mit einer Wahrscheinlichkeit von 97% wird die Maschine in den nÃ¤chsten 1-3 Tagen ausfallen, laut unserem Modell. Die Investition lohnt sich \\(\\Leftrightarrow\\) Die Investition hat einen Erwartungswert von 42 Euro; mit 90% Wahrscheinlichkeit wird der Gewinn zwischen -10000 und 100 Euro. ğŸ‹ Geben Sie weitere Beispiele an! 2.3.3 Visualisierung von PunktschÃ¤tzungen Rot markiert: Die PunktschÃ¤tzung von mpg fÃ¼r hp=200. ğŸ‹ Geben Sie ein vergleichbares Beispiel an! 2.3.4 Die PunktschÃ¤tzung berÃ¼cksichtigt nicht die Ungewissheit des Models Zwei Arten von Ungewissheit mÃ¼ssen wir (mindestens) in unseren Vorhersagen berÃ¼cksichtigen: zur Lage der Regressionsgeraden (\\(\\beta_0\\), \\(\\beta_1\\)) zu EinflÃ¼ssen, die unser Modell nicht kennt (\\(\\epsilon, \\sigma\\)) 2.3.4.1 Unsicherheit in \\(\\beta_0, \\beta1\\) 2.3.4.2 Unsicherheit durch \\(\\epsilon\\) (\\(\\sigma\\)) 2.3.5 Vorhersage-Intervall: berÃ¼cksichtigt Ungewissheit in \\(\\beta_0, \\beta_1, \\epsilon\\) Das Vorhersage-Intervall berÃ¼cksichtigt Ungewissheit in \\(\\beta_0, \\beta_1, \\epsilon\\) bei der Vorhersage von \\(\\hat{y_i}\\). ğŸ‹ Interpretieren Sie den Ungewissheitskorridor! 2.4 Klassische vs.Â Bayes-Inferenz 2.4.1 Klassische Inferenz: Frequentismus Die BerÃ¼cksichtigung von Vorwissen zum Sachgegenstand wird vom Frequentismus als subjektiv zurÃ¼ckgewiesen. Nur die Daten selber fliesen in die Ergebnisse ein Wahrscheinlichkeit wird Ã¼ber relative HÃ¤ufigkeiten definiert. Es ist nicht mÃ¶glich, die Wahrscheinlichkeit einer Hypothese anzugeben. Stattdessen wird angegeben, wie hÃ¤ufig eine vergleichbare Datenlage zu erwarten ist, wenn die Hypothese gilt und der Versuch sehr hÃ¤ufig wiederholt ist. Ein GroÃŸteil der Forschung (in den Sozialwissenschaften) verwendet diesen Ansatz. 2.4.2 Bayesianische Inferenz Vorwissen (Priori-Wissen) flieÃŸt explizit in die Analyse ein (zusammen mit den Daten). Wenn das Vorwissen gut ist, wird die Vorhersage genauer, ansonsten ungenauer. Die Wahl des Vorwissens muss explizit (kritisierbar) sein. In der Bayes-Inferenz sind Wahrscheinlichkeitsaussagen fÃ¼r Hypothesen mÃ¶glich. Die Bayes-Inferenz erfordert mitunter viel Rechenzeit und ist daher erst in den letzten Jahren (fÃ¼r gÃ¤ngige Computer) komfortabel geworden. 2.4.3 Vergleich von Wahrscheinlichkeitsaussagen 2.4.3.1 Frequentismus zentrale Statistik: p-Wert â€œWie wahrscheinlich ist der Wert der Teststatistik (oder noch extremere Werte), vorausgesetzt die Nullhypothese gilt und man wiederholt den Versuch unendlich oft (mit gleichen Bedingungen, aber zufÃ¤llig verschieden und auf Basis unseres Modells)?â€ 2.4.3.2 Bayes-Statistik zentrale Statistik: Posteriori-Verteilung â€œWie wahrscheinlich ist die Forschungshypothese, jetzt, nachdem wir die Daten kennen, auf Baiss unseres Modells?â€ ğŸ‹ Recherchieren Sie eine Definition des p-Werts und lesen Sie sie genau. 2.4.4 Frequentist und Bayesianer Quelle 2.4.5 Der p-Wert ist wenig intuitiv from Imgflip Meme Generator 2.4.6 Beispiel zum Nutzen von Apriori-Wissen 1 Ein Betrunkener behauptet, er kÃ¶nne hellsehen. Er wirft eine MÃ¼nze 10 Mal und sagt jedes Mal korrekt vorher, welche Seite oben landen wird. Die Wahrscheinlichkeit dieses Ergebnisses ist sehr gering (\\(2^{-10}\\)) unter der Hypothese, dass die MÃ¼nze fair ist, dass Ergebnis also â€œzufÃ¤lligâ€ ist. Unser Vorwissen lÃ¤sst uns allerdings trotzdem an der Hellsichtigkeit des Betrunkenen zweifeln, so dass die meisten von uns die Hypothese von der ZufÃ¤lligkeit des Ergebnisses wohl nicht verwerfen. 2.4.7 Beispiel zum Nutzen von Apriori-Wissen 2 Eine Studie fand einen â€œgroÃŸen Effektâ€ auf das Einkommen von Babies, eine Stunde pro Woche wÃ¤hrend zwei Jahren an einem psychosozialen Entwicklungsprogramm teilnahmen (im Vergleich zu einer Kontrollgruppe), \\(n=127\\). Nach 20 Jahren war das mittlere Einkommen der Experimentalgruppe um 42% hÃ¶her (als in der Kontrollgruppe) mit einem Konfidenzintervall von [+2%,+98%]. Allerdings lÃ¤sst uns unser Vorwissen vermuten, dass so ein Treatment das Einkommen nach 20 Jahren kaum verdoppeln lÃ¤sst. Wir wÃ¼rden den Effekt lieber in einem konservativeren Intervall schÃ¤tzen (enger um Null). 2.5 Literatur oft mit Pr oder p abgekÃ¼rzt, fÃ¼r probabilityâ†©ï¸ "],["ungewissheit-quantifizieren.html", "Kapitel 3 Ungewissheit quantifizieren 3.1 Lernsteuerung 3.2 Was ist Wahrscheinlichkeit? 3.3 Zufallsexperiment 3.4 Additionsregel 3.5 UnabhÃ¤ngigkeit zweier Ereignisse 3.6 Multiplikationsregel fÃ¼r unabhÃ¤ngige Ereignisse 3.7 Hintergrundwissen 3.8 Bedingte Wahrscheinlichkeit", " Kapitel 3 Ungewissheit quantifizieren 3.1 Lernsteuerung 3.1.1 Lernziele Wahrscheinlichkeit definieren und relevante Begriffe anfÃ¼hren und in GrundzÃ¼gen erklÃ¤ren einfache Fragen aus der Wahrscheinlichkeitstheorie berechnen 3.1.2 Literatur NA 3.2 Was ist Wahrscheinlichkeit? Die Wahrscheinlichkeit \\(p\\) quantifiziert Ungewissheit im Hinblick auf eine Aussage bzw. ein Ereignis \\(A\\), gegeben eines Hintergrundwissen \\(H\\). \\(p=0\\) heiÃŸt, wir halten die Aussage (das Ereignis) fÃ¼r falsch (unmÃ¶glich); \\(p=1\\) heiÃŸt, wir halten die Aussage (das Ereignis) fÃ¼r wahr (sicher). \\(0&lt;p&lt;1\\) heiÃŸt, wir sind (mehr oder weniger) unsicher bzgl. der Aussage bzw. ob das Ereignis zutrifft. \\(p(\\text{Kopf werfen mit einer fairen MÃ¼nze}) = 1/2\\). \\(p(\\text{eine 6 wÃ¼rfeln mit einer fairen WÃ¼rfel}) = 1/6\\). \\(p(\\text{Entweder ist heute Montag oder nicht}) = 1\\). \\(p(\\text{Berlin ist die Hauptstadt von Frankreich}) = 0\\). ğŸ‹ Weitere Beispiele? 3.3 Zufallsexperiment Als Zufallsexperiment bezeichnen wir einen Vorgang, bei dem wir wissen, was alles passieren kÃ¶nnte, aber nicht sicher sind, was genau passiert. Die Menge der mÃ¶glichen Ergebnisse nennt man Grundraum (Ergebnisraum) \\(\\Omega\\). Beim WÃ¼rfelwurf: \\(\\Omega = \\{1, 2, 3, 4, 5, 6\\}\\) Jede Teilmenge \\(A \\subseteq \\Omega\\) nennt man ein Ereignis. Beim WÃ¼rfelwurf: z.B. \\(A = \\{2, 4, 6\\}\\), eine gerade Zahl werfen. Ein Ereignis, das genau ein Element enthÃ¤lt, heiÃŸt Elementarereignis. Ein Ereignis, das alle Elementarereignisse aus \\(\\Omega\\) enthÃ¤lt, die nicht zum Ereignis \\(A\\) gehÃ¶ren, nennt man das KomplementÃ¤rereignis (Komplement) \\(A^C\\) (auch: \\(\\overline{A}, \\neg A\\)). Beim WÃ¼rfelwurf: Das Komplement von \\(A=\\{2,4,6\\}\\) ist \\(A^C=\\{1,3,5\\}\\), die ungeraden Zahlen. ğŸ‹ Beschreiben Sie ein weiteres Zufallsexperiment! 3.4 Additionsregel Die Wahrscheinlichkeit, dass mindestens eines der beiden sich ausschlieÃŸenden Ereignissen \\(A\\) und \\(B\\) der Fall ist, ist durch die Additionsregel gegeben: \\(Pr(\\text{A oder B}) = Pr(A \\cup B) = Pr(A) + Pr(B)\\) Beispiel: Wahrscheinlichkeit mit einem â€œfairenâ€ WÃ¼rfelwurf \\(X\\) eine 1 oder 2 zu werfen: \\(Pr(X=1 \\cup X=2) = Pr(X=1) + Pr(X=2) = 1/6 + 1/6 = 1/3\\) ğŸ‹ Was ist \\(Pr(X &lt; 4)\\), \\(Pr(1 \\le X \\le 6)\\)? 3.5 UnabhÃ¤ngigkeit zweier Ereignisse Zwei Ereignisse sind (stochastisch) unabhÃ¤ngig, wenn Kenntnis des einen uns keine Information gibt, ob das andere der Fall ist. Ansonsten nennt man die beiden Ereignisse (stochastisch) abhÃ¤ngig oder zusammenhÃ¤ngend. Angenommen wir werfen zwei faire MÃ¼nzen. Wir wissen, die erste MÃ¼nze zeigt Kopf. Dieses Wissen gibt uns keine weitere Information, welche Seite bei der zweiten MÃ¼nze oben liegt. Auf der anderen Seite sind Aktienkurs hÃ¤ufig voneinander abhÃ¤ngig. WeiÃŸ man, dass eine Aktie gestiegen ist, so ist es (hÃ¤ufig) wahrscheinlich, dass die andere auch gestiegen ist. Achtung: Stochastische (Un)abhÃ¤ngigkeit impliziert nicht kausale (Un)abhÃ¤ngigkeit. 3.5.1 Beispiele fÃ¼r abhÃ¤ngige und unabhÃ¤ngige Ereignisse \\(A\\) und \\(B\\) 3.5.2 UnabhÃ¤ngig MÃ¼nzwurf 1 (A) und MÃ¼nzwurf 2 (B), jeweils fair Meine Stimmung (A) und ob das heutige Datum eine Primzahl ist (B) Zwei Passanten getrennt zu ihrer Meinung zu einem politischen Thema befragen Die KÃ¶rpergrÃ¶ÃŸe zweier zufÃ¤llig gezogener Personen (A und B) 3.5.3 AbhÃ¤ngig KÃ¶rpergrÃ¶ÃŸe zweier Geschwister (A und B) Lernleistung zwier SchÃ¼leris A und B der gleichen Klasse PS-Zahl (A) und Spritverbrauch (B) Augenzahl beim zweimaligen Wurf (A und B) eines gezinkten WÃ¼rfels Geschlecht (A) und ob die Person Papst ist (B) 3.6 Multiplikationsregel fÃ¼r unabhÃ¤ngige Ereignisse Seien \\(A\\) und \\(B\\) zwei unabhÃ¤ngige Ereignisse, dann nennt man die gemeinsame Wahrscheinlichkeit \\(Pr(AB)\\), die Wahrscheinlichkeit, dass beide Ereignisse eintreten. Sie berechnet sich als Produkt der jeweiligen Wahrscheinlichkeiten von \\(A\\) und \\(B\\): \\[Pr(A \\text{ und } B ) = Pr(AB) = Pr(A \\cap B) = Pr(A) \\cdot Pr(B)\\] Wirft man zwei faire MÃ¼nzen2, so ist die Wahrscheinlichkeit, dass beide Kopf zeigen: \\(Pr(KK) = Pr(K) \\cdot Pr(K) = 1/2 \\cdot 1/2 = 1/4\\). ğŸ‹ Was ist \\(Pr(ZZ)\\)? Ist \\(Pr(ZK) = Pr(KZ)\\)? 3.6.1 Beispiele fÃ¼r die Multiplikationsregel unabhÃ¤ngiger Ereignisse Zwei Mal hintereinander eine 6 werfen (fairer WÃ¼rfel): \\(Pr(6, 6) = Pr(6) \\cdot Pr(6) = 1/6 \\cdot 1/6 = 1/36\\). Beim Lotto wird erst die Zahl \\(42\\) und dann die Zahl \\(1\\) gezogen: \\(Pr(42,1)=1/49 \\cdot 1/48 \\approx 4.3\\times 10^{-4}\\). Bei der Klausur alle 10 Richtig-Falsch-Fragen richtig zu raten: \\(Pr(10r) = 1/2^{10} \\approx 0.001\\approx 1/1000\\) . Wenn man in der Disko 10 Personen anspricht, Korb-Wahrscheinlichkeit \\(p(K)=9/10\\) betrÃ¤gt, wie hoch ist die Wahrscheinlichkeit nicht alleine nach hause zugehen? \\(Pr(\\neg0) = 1-0.9^{10} \\approx 0.65132\\). Ei Forschi fÃ¼hrt 10 statistische Tests durch, jeweils mit 10% Gefahr, dass ein falsch-positives Ergebnis zustande kommt. Wie hoch ist die Wahrscheinlichkeit fÃ¼r mindestens 1 falsch-positives Ergebnis? \\(Pr(\\neg 0 FP) = 1 - 0.9^{10} \\approx 0.65\\) 3.6.2 MÃ¼nzen werfen als Baum: Anzahl Kopf nach 2 WÃ¼rfen Ereignis Pr 0K 1/2 * 1/2 = 1/4 1K 1/4 + 1/4 = 1/2 2K 1/2 * 1/2 = 1/4 ğŸ‹ Zeichnen Sie den Baum und berechnen Sie die Wahrscheinlichkeiten fÃ¼r eine gezinkte MÃ¼nze mit \\(P(K) = 2/3\\). 3.6.3 MÃ¼nzen werfen als Baum: Anzahl Kopf nach 3 WÃ¼rfen Ereignis Pr 0K 1/2 * 1/2 * 1/2 = 1/8 1K 1/8 + 1/8 + 1/8 = 3/8 2K 3 * 1/8 = 3/8 3K 1/2 * 1/2 * 1/2 = 1/8 3.7 Hintergrundwissen 3.7.1 Wahrscheinlichkeit ist abhÃ¤ngig vom Hintergrundwissen (\\(H\\)) \\(Pr(A|H)\\): Die Wahrscheinlichkeit von \\(A\\), gegeben \\(H\\). A: â€œSokrates ist sterblich.â€; H: â€œAlle Menschen sind sterblich und Sokrates ist ein Mensch.â€ \\(\\implies Pr(A|H) = 1\\). A: â€œDie MÃ¼nze zeigt Kopfâ€; H: â€œWir haben keinen Grund anzunehmen, dass eine der beiden Seiten hÃ¤ufiger oben liegt oder das sonst etwas passiert.â€ \\(\\implies Pr(A|H)=1/2\\). A: â€œSchorsch, das rosa Einhort, mag Bier.â€; H: â€œ50% der rosa EinhÃ¶rner mÃ¶gen Bier.â€ \\(\\implies Pr(A|H) = 1/2\\). Die Wahrscheinlichkeit eine 6 zu wÃ¼rfeln (A), gegeben dem Hintergrundwissen (H), dass der WÃ¼rfel â€œfairâ€ ist, d.h. wir kein Wissen haben, dass eine Augenzahl hÃ¤ufiger auftritt, ist \\(1/6\\). 3.7.2 Hintergrundwissen ist subjektiv Ich habe gerade einen Stift in meiner Hosentasche (links oder rechts). Wie groÃŸ ist die Wahrscheinlichkeit, dass der Stift in meiner linken Tasche ist (und nicht in der rechten)? Bezogen auf Ihr Hintergrundwissen gilt: \\(Pr(\\text{A=&quot;Stift links&quot;|H=&quot;kein besonderes Wissen zu der Frage&quot;}) = 1/2\\). Bezogen auf mein Hintergrundwissen gilt: \\(Pr(\\text{A=&quot;Stift links&quot;|H=&quot;Der Stift ist links&quot;}) = 1\\). Briggs (2016) ğŸ‹ Geben Sie ein weiteres Beispiel fÃ¼r die SubjektivitÃ¤t von Hintergrundwissen an! Formalisieren Sie es wie oben gezeigt. 3.8 Bedingte Wahrscheinlichkeit Wie groÃŸ ist die Wahrscheinlichkeit: die Klausur zu bestehen, wenn man gelernt hat? von schlechter Laune, gegeben es ist Montag? schwer an Corona zu erkranken, unter der Bedingung, man ist geimpft? \\(Pr(A|H)\\) ist die Wahrscheinlichkeit, dass \\(A\\) eintritt, gegeben bzw. unter der Bedingung, dass \\(H\\) eingetreten ist. Formel der bedingten Wahrscheinlichkeit: \\[Pr(B|l) = \\frac{Pr(B \\cap l)}{Pr(l)}\\] 3.8.1 Kontingenztabelle zur Berechnung von Wahrscheinlichkeiten Beispiel aus den Klausurergebnissen bei Prof.Â SÃ¼ÃŸ: . bestanden (B) nicht (Â¬B) SUMME hat gelernt (l) 36 6 42 nicht (Â¬l) 12 24 36 SUMME 48 30 78 Randwahrscheinlichkeit: \\(Pr(B) = 48/78 \\approx 0.61 \\quad Pr(l) = 42/78 \\approx 0.54\\) Bedingte Wahrscheinlichkeit: \\(Pr(B|l) = 36/42 \\approx \\frac{0.46}{0.54} \\approx 0.86 \\quad Pr(l|B) = 36/48 = 0.75\\) Gemeinsame Wahrscheinlichkeit: \\(Pr(B \\cap l) = Pr(l \\cap B) = Pr(Bl) = Pr(lB) = 36/78 \\approx 0.46\\) 3.8.2 Visualierung von gemeinsamer und bedingter Wahrscheinlichkeit \\(Pr(AB) = Pr(A) \\cdot Pr(B) = 50\\% \\cdot 50\\% = 25\\%\\) \\(Pr(A|B) = Pr(A,B) / Pr(B) = 25\\% / 50\\% = 50\\%\\) 3.8.3 Visualierung von (un)abhÃ¤ngigen Ereignissen Ã„ndert sich die Wahrscheinlichkeit eines Ereignisses, wenn man es auf ein anderes bedingt, so sind beide Ereignisse abhÃ¤ngig, ansonsten unabhÃ¤ngig. 3.8.3.1 AbhÃ¤ngig \\(P(A|B) \\ne Pr(A) \\ne Pr(A|\\neg B)\\) Ãœberleben auf der Titanic ist abhÃ¤ngig von der Passagierklasse. 3.8.3.2 UnabhÃ¤ngig \\(P(A|B) = Pr(A) = Pr(A|\\neg B)\\) Ãœberleben auf der Titanic ist unabhÃ¤ngig vom Ereignis Alter ist eine Primzahl. 3.8.4 Beispiel zur Visualisierung zweier abhÃ¤ngiger Ereignisse Sind die Ereignisse Tod durch Covid bzw. Impfquote (\\(A\\)) und Land3 (\\(B\\)) voneinander abhÃ¤ngig? Ja, da in beiden Diagrammen gilt: \\(P(A|B) \\ne Pr(A) \\ne Pr(A|\\neg B)\\). Daten von Our World in Data Hannah Ritchie and Roser (2020) References "],["bayes.html", "Kapitel 4 Bayes 4.1 Lernsteuerung 4.2 Kleine Welt, groÃŸe Welt 4.3 Bayes-Statistik als ZÃ¤hlen 4.4 Ein erstes Modell 4.5 Der datengenierende Prozess: Wie entstanden die Daten? 4.6 Bayes berechnen mit R 4.7 Literatur", " Kapitel 4 Bayes 4.1 Lernsteuerung 4.1.1 Lernziele Wahrscheinlichkeit definieren und relevante Begriffe anfÃ¼hren und in GrundzÃ¼gen erklÃ¤ren einfache Fragen aus der Wahrscheinlichkeitstheorie berechnen 4.1.2 Literatur NA 4.2 Kleine Welt, groÃŸe Welt 4.2.1 Behaims Globus, Kolumbus glÃ¼cklicher Fehler Quelle 4.2.2 Kleine Welt vs.Â groÃŸe Welt 4.2.2.1 Kleine Welt Die Welt, wie sie der Golem sieht entspricht dem Modell (zwanglÃ¤ufig) 4.2.2.2 GroÃŸe Welt Die Welt, wie sie in Wirklichkeit ist entspricht nicht (zwangslÃ¤ufig) dem Modell Behaims Globus ist nicht gleich der Erde. Die kleine Welt ist nicht die groÃŸe Welt. Was in der kleinen Welt funktioniert, muss nicht in der groÃŸen Welt funktionieren. Modelle zeigen immer nur die kleine Welt: Vorsicht vor schnellen SchlÃ¼ssen und vermeintlicher Gewissheit. ğŸ‹ Nennen Sie ein Beispiel, in dem ein Modell nicht (exakt) der Wirklichkeit entspricht! 4.2.3 So denkt unser Bayes-Golem ğŸ‹ Bayes-Inferenz Ã¤hnelt dem Lernen von Menschen. Geben Sie ein Beispiel von Lernen bei Menschen, das oben dargestelltem Prozess Ã¤hnelt! 4.3 Bayes-Statistik als ZÃ¤hlen 4.3.1 Murmeln im SÃ¤ckchen Sie haben ein SÃ¤ckchen mit vier Murmeln darin. Sie wissen nicht, welche Farben die Murmeln haben. Murmeln gibtâ€™s in zwei Farben: weiÃŸ (W) oder blau (B). Es gibt daher fÃ¼nf Hypothesen zur Farbe der Murmeln im SÃ¤ckchen: [WWWW], [BWWW], [BBWW], [BBBW], [BBBB.] Unser Ziel ist, die Wahrscheinlichkeiten der Hypothesen nach Ziehen von Murmeln zu bestimmen. 4.3.2 Unsere Daten Wir ziehen eine Murmel, merken uns die Farbe und legen sie zurÃ¼ck. Das wiederholen wir noch zwei Mal (Ziehen mit ZurÃ¼cklegen). Wir erhalten: BWB. VoilÃ : unsere Daten. (Kurz 2021) ğŸ‹ Wie groÃŸ ist die Stichprobe (\\(N\\))? Ist die Wahrscheinlichkeit fÃ¼r B in jedem Zug gleich? 4.3.3 ZugmÃ¶glichkeiten laut Hypothese [BWWW], 1. Zug Wenn Hypothese [BWWW] der Fall sein sollte, dann kÃ¶nnen wir im ersten Zug entweder die eine blaue Murmel erwischen oder eine der drei weiÃŸen. Nachdem wir die Murmel gezogen haben (und die Farbe gemerkt haben), legen wir sie wieder ins SÃ¤ckchen: Ziehen mit ZurÃ¼cklegen. ğŸ‹ Wie viele Elementarereignisse hat dieses Zufallsexperiment? Sind alle gleich wahrscheinlich? 4.3.4 ZugmÃ¶glichkeiten laut Hypothese [BWWW], 1. und 2. Zug Wenn Hypothese [BWWW] der Fall sein sollte, dann haben wir im zweiten Zug natÃ¼rlich die gleichen MÃ¶glichkeiten wie im ersten. Zug 1 und Zug 2 zusammen genommen gibt es \\(16=4\\cdot4=4^2\\) Kombinationen an gezogenen Murmeln: ğŸ‹ Ist jedes Elementarereignis (z.B. BB, BW,â€¦) gleich wahrscheinlich? 4.3.5 ZugmÃ¶glichkeiten laut Hypothese [BWWW], 1.-3. Zug Zug 1, Zug 2 und Zug 3 zusammen genommen, gibt es dann \\(4\\cdot4\\cdot4=4^3=64\\) Kombinationen, drei Murmeln zu ziehen. ğŸ‹ Wie wahrscheinlich ist ein bestimmtes dieser 64 Ereignisse (unter der Annahme gleicher Wahrscheinlichkeit)? 4.3.6 Welche ZÃ¼ge sind logisch mÃ¶glich? Einige Kombinationen (â€œPfadeâ€) der Hypothese [BWWW] lassen sich nicht mit unseren Daten (BWB) vereinbaren. Z.B. alle Kombinationen die mit W beginnen, sind nicht mit unseren Daten zu vereinbaren. Nur 3 der 64 â€œPfadeâ€ (Kombinationen), die Hypothese [BWWW] vorgibt, sind mit unseren Daten logisch zu vereinbaren. 4.3.7 Kombinationen fÃ¼r Hypothesen Hypothese HÃ¤ufigkeit BWB [W W W W] 0 * 4 * 0 = 0 [B W W W] 1 * 3 * 1 = 3 [B B W W] 2 * 2 * 2 = 8 [B B B W] 3 * 1 * 3 = 9 [B B B B] 4 * 0 * 4 = 0 Die HÃ¤ufigkeiten der Kombinationen (Pfade) ist proportional zur PlausibilitÃ¤t einer Hypothese: Je mehr Pfade laut Hypothese, desto wahrscheinlicher die Hypothese (unter sonst gleichen Bedingungen). ZusÃ¤tzlich mÃ¼ssten wir noch beachten, ob bestimmte Hypothesen per se bzw. a priori wahrscheinlicher sind. So kÃ¶nnten blaue Murmeln selten sein. Gehen wir der Einfachheit halber zunÃ¤chst davon aus, dass alle Hypothesen apriori gleich wahrscheinlich sind. 4.3.8 Pfadbaum fÃ¼r die Hypothesen [BWWW], [BBWW], [BBBW] 4.3.9 Wir ziehen einer vierte Murmel: B Gehen wir zunÃ¤chst davon aus, dass alle Hypothesen apriori gleich wahrscheinlich sind. Wir ziehen wieder eine Murmel. Sie ist blau (B)! Jetzt kÃ¶nnten wir den Pfadbaum fÃ¼r vier (statt drei) ZÃ¼ge aufmalen. Oder wir machen ein Update: Wir aktualisieren die bisherigen KombinationshÃ¤ufigkeiten um die neuen Daten. Die alten Daten dienen dabei als Priori-Informationen fÃ¼r die neuen Daten. 4.3.10 Priori-Information nutzen Mit den Daten BWBB ist die Hypothese [BBBW] am wahrscheinlichsten: Hyp PB HA HN [W W W W] 0 0 0 * 0 = 0 [B W W W] 1 3 1 * 3 = 3 [B B W W] 2 8 2 * 8 = 16 [B B B W] 3 9 3 * 9 = 27 [B B B B] 4 0 4 * 0 = 0 Hyp: Hypothese PB: Anzahl von Pfaden fÃ¼r B HA: alte (bisherige) HÃ¤ufigkeiten HN: neue (geupdatete) HÃ¤ufigkeiten 4.3.11 Murmelfabrik streikt: Blaue Murmeln jetzt sehr selten! BerÃ¼cksichtigen wir jetzt die Information, dass apriori (bevor wir die Daten gesehen haben), einige Hypothesen wahrscheinlicher (plausibler) sind als andere. Hier ist die Hypothese [BBWW] am wahrscheinlichsten: Hyp HA HF HN [W W W W] 0 0 0 * 0 = 0 [B W W W] 3 3 3 * 3 = 9 [B B W W] 16 2 16 * 2 = 32 [B B B W] 27 1 27 * 1 = 27 [B B B B] 0 0 0 * 0 = 0 HF: HÃ¤ufigkeit des SÃ¤ckchentyps laut Fabrik. 4.3.12 ZÃ¤hlen mit groÃŸen Zahlen nervt Malen Sie mal den Pfadbaum fÃ¼r 10 ZÃ¼ge â€¦ Eine Umrechnung der HÃ¤ufigkeiten in Anteile macht das Rechnen einfacher. Dazu definieren wir die geupdatete PlausibilitÃ¤t einer Hypothese nach Kenntnis der Daten: \\[\\text{PlausibilitÃ¤t von [BWWW] nach Kenntnis von BWB}\\] \\[\\propto\\] \\[\\text{Anzahl mÃ¶glicher Pfade bei [BWWW] fÃ¼r BWB}\\] \\[\\times\\] \\[\\text{Priori-PlausibilitÃ¤t von [BWWW]}\\] \\(\\propto\\): proportional zu 4.3.13 PlausibilitÃ¤t berechnen Sei \\(p\\) der Anteil blauer Murmeln. Bei Hypothese [BWWW] gilt, dann ist \\(p=1/4 = 0.25\\). Sei \\(D_{neu} =\\) BWB, die Daten: \\[\\text{PlausibilitÃ¤t von }p\\text{ nach Kenntnis von }D_{neu}\\] \\[\\propto\\] \\[\\text{Anzahl Pfade von }p\\text{ fÃ¼r }D_{neu}\\] \\[\\times\\] \\[\\text{Priori-PlausibilitÃ¤t von }p\\] FÃ¼r jeden Wert von \\(p\\) beurteilen wir dessen PlausibilitÃ¤t als umso hÃ¶her, je mehr Pfade durch den Pfadbaum fÃ¼hren und je hÃ¶her die PlausibilitÃ¤t des Werts von \\(p\\) von vornherein ist. 4.3.14 Von PlausibilitÃ¤t zur Wahrscheinlichkeit Teilen wir die Anzahl Pfade einer Hypothese durch die Anzahl aller Pfade (aller Hypothesen), so bekommen wir einen Anteil. Damit haben wir eine Wahrscheinlichkeit: \\[\\text{Pl von }p\\text{ mit Daten }D_{neu} =\\] \\[\\frac{\\text{Anzahl Pfade von }p\\text{ fÃ¼r }D_{neu}\\times \\text{Prior-Pl von }p}{\\text{Summe aller Pfade}}\\] Pl: PlausibilitÃ¤t ğŸ‹ Was muss passieren, dass der Bruch gleich Null ist? 4.3.15 PlausibilitÃ¤t pro Hypothese source(&quot;R-Code/tab24.R&quot;) Hyp p AP Pl [W W W W] 0.00 0 0.00 [B W W W] 0.25 3 0.15 [B B W W] 0.50 8 0.40 [B B B W] 0.75 9 0.45 [B B B B] 1.00 0 0.00 p: Anteil blauer Murmeln (Priori-Wissen) AP: Anzahl von mÃ¶glichen Pfaden; Pl: PlausibilitÃ¤t AP &lt;- c(0, 3, 8, 9, 0) Pl &lt;- AP / sum(AP) Pl ## [1] 0.00 0.15 0.40 0.45 0.00 4.3.16 Fachbegriffe Kennwerte laut einer Hypothese, wie den Anteil blauer Murmeln \\(p\\) bezeichnet man als Parameter. Den Anteil gÃ¼ltiger Pfade pro Hypothese (bzw. pro Wert von \\(p\\)) bezeichnet man als Likelihood. Die Priori-PlausibilitÃ¤t nennt man Priori-Wahrscheinlichkeit. Die neue, geupdatete PlausibilitÃ¤t fÃ¼r einen bestimmten Wert von \\(p\\) nennt man Posteriori-Wahrscheinlichkeit. ğŸ‹ ErklÃ¤ren Sie die Begriffe dem nÃ¤chsten Menschen, den Sie treffen! 4.3.17 Zusammenfassung Schritt: Unser Vorab-Wissen zur Wahrscheinlichkeit jeder Hypothese wird mit dem Begriff Priori-Verteilung gefasst. Schritt: Wir zÃ¤hlen den Anteil gÃ¼ltiger Pfade fÃ¼r jede Hypothese; d.h. wir berechnen den Likelihood jeder Hypothese. Schritt: Mit den Likelihoods updaten wir unsere Priori-Verteilung. Die Wahrscheinlichkeit jeder Hypothese verÃ¤ndert sich entsprechend der Daten. Es resultiert die Posteriori-Verteilung. 4.4 Ein erstes Modell 4.4.1 Welcher Anteil der ErdoberflÃ¤che ist mit Wasser bedeckt? knitr::include_graphics(&quot;img/earth.png&quot;) Quelle CC 4.0 BY-NC Sie werden einen Globus-Ball in die Luft und fangen in wieder auf. Sie notieren dann, ob die Stelle unter Ihrem Zeigefinger Wasser zeigt (W) oder Land (L). Den Versuch wiederholen Sie 9 Mal. \\[W \\quad L \\quad W \\quad W \\quad W \\quad L \\quad W \\quad L \\quad W\\] ğŸ‹ Besorgen Sie sich einen Globus (zur Not eine MÃ¼nze) und stellen Sie den Versuch nach! 4.5 Der datengenierende Prozess: Wie entstanden die Daten? Der wahre Anteil von Wasser der ErdoberflÃ¤che ist \\(p\\). Ein Wurf des Globusballes hat die Wahrscheinlichkeit \\(p\\), eine \\(W\\)-Beobachtung zu erzeugen. Die WÃ¼rfe des Globusballes sind unabhÃ¤ngig voneinander. Wir haben kein Vorwissen Ã¼ber \\(p\\); jeder Wert ist uns gleich wahrscheinlich. ğŸ‹ Welche Annahmen wÃ¼rden Sie Ã¤ndern? Welche kÃ¶nnte man wegnehmen? Welche hinzufÃ¼gen? Was wÃ¤ren die Konsequenzen? 4.5.1 Wissen updaten: Wir fÃ¼ttern Daten in das Modell 4.5.2 Erinnern wir uns an das Urnen-Beispiel FÃ¼r jede Hypothese haben wir ein Vorab-Wissen, das die jeweilige PlausibilitÃ¤t der Hypothese angibt: Priori-Verteilung. FÃ¼r jede Hypothese (d.h. jeden Parameterwert \\(p\\)) mÃ¶chten wir den Anteil (die Wahrscheinlichkeit) gÃ¼ltiger Kombinationen wissen. Das gibt uns den Likelihood. Dann gewichten wir den Likelihood mit dem Vorabwissen, so dass wir die Posteriori-Verteilung4 bekommen. 4.5.3 Die Binomialverteilung Wir nehmen an, dass die Daten unabhÃ¤ngig voneinander entstehen und sich der Parameterwert nicht zwischenzeitlich Ã¤ndert. Dann kann man die Wahrscheinlichkeit (\\(Pr\\)), \\(W\\) mal Wasser und \\(L\\) mal Land zu beobachten, wenn die Wahrscheinlichkeit fÃ¼r Wasser \\(p\\) betrÃ¤gt, mit der Binomialverteilung berechnen. Die Binomialverteilung zeigt die Verteilung der HÃ¤ufigkeit (Wahrscheinlichkeit) der Ereignisse (z.B. 2 Mal Kopf) beim wiederholten MÃ¼nzwurf (und allen vergleichbaren Zufallsexperimenten)5. \\[Pr(W,L|p) = \\frac{(W+L)!}{W!L!}p^W(1-p)^L\\] 4.5.4 Binomialverteilung mit R Was ist der Anteil der gÃ¼ltigen Pfade (Wahrscheinlichkeit), um 6 mal \\(W\\) bei \\(N=W+L=9\\) WÃ¼rfen zu bekommen, wenn wir von \\(p=1/2\\) ausgehen? dbinom(x = 6, size = 9, prob = 1/2) ## [1] 0.1640625 Was ist die Wahrscheinlichkeit fÃ¼r \\(W=9\\) bei \\(N=9\\) und \\(p=1/2\\)? dbinom(x = 9, size = 9, prob = 1/2) ## [1] 0.001953125 4.5.5 Beispiele zur Berechnung einer binomial verteilten Wahrscheinlichkeit Ei Professori stellt einen Klausur mit 20 Richtig-Falsch-Fragen. Wie groÃŸ ist die Wahrscheinlichkeit, durch bloÃŸes MÃ¼nze werfen genau 15 Fragen richtig zu raten?6 dbinom(x = 15, size = 20, prob = .5) ## [1] 0.01478577 Was ist die Wahrscheinlichkeit bei 3 MÃ¼nzwÃ¼rfen (genau) 3 Treffer (Kopf) zu erzielen? dbinom(3, 3, 1/2) ## [1] 0.125 4.5.6 Unser Modell ist geboren Wir fassen das Globusmodell so zusammen: \\[W \\sim \\text{Bin}(N,p),\\] Lies: â€œW ist binomial verteilt mit den Parametern \\(N\\) und \\(p\\)â€. \\(N\\) gibt die Anzahl der GlobuswÃ¼rfe an: \\(N=W+L\\). Unser Vorab-Wissen zu \\(p\\) sei, dass uns alle Werte gleich plausibel erscheinen (â€œuniformâ€): \\[p \\sim \\text{Unif}(0,1).\\] Lies: â€œ\\(p\\) ist gleich (uniform) verteilt mit der Untergrenze 0 und der Obergrenze 1â€. 4.5.7 So sehen die Verteilungen aus 4.5.7.1 Binomialverteilung \\(N=9, p = 1/2\\) 4.5.7.2 Gleichverteilung \\(Min = 0, Max = 1\\) ğŸ‹ Was fÃ¤llt Ihnen bei der Binomialverteilung auf? Ist sie symmetrisch? VerÃ¤ndert sich die Wahrscheinlichkeit linear? Was fÃ¤llt Ihnen bei der Gleichverteilung auf? 4.5.8 Herleitung Bayesâ€™ Theorem 1/2: Gemeinsame Wahrscheinlichkeit Die Wahrscheinlichkeit fÃ¼r Regen und kalt ist gleich der Wahrscheinlihckeit von Regen, gegeben kalt mal der Wahrscheinlicht von kalt. Entsprechend gilt: Die Wahrscheinlichkeit von \\(W\\), \\(L\\) und \\(p\\) ist das Produkt von \\(Pr(W,L|p)\\) und der Prior-Wahrscheinlichkeit \\(Pr(p)\\): \\[Pr(W,L,p) = Pr(W,L|p) \\cdot Pr(p)\\] Genauso gilt: Die Wahrscheinlichkeit von Regen und kalt ist gleich der Wahrscheinlichkeit kalt, wennâ€™s regnet mal der Wahrscheinlichkeit von Regen: \\[Pr(W,L,p) = Pr(p|W,L) \\cdot Pr(W, L)\\] 4.5.9 Herleitung Bayesâ€™ Theorem 2/2: Posteriori-Wahrscheinlichkeit Wir setzen die letzten beiden Gleichungen gleich: \\[Pr(W,L|p) \\cdot Pr(p) = Pr(p|W,L) \\cdot (W,L)\\] Und lÃ¶sen auf nach der Posteriori-Wahrscheinlichkeit, \\(Pr(p|W,L)\\): \\[Pr(p|W,L) = \\frac{Pr(W,L|p) Pr(p)}{Pr(W,L)}\\] \\(Pr(W,L)\\) nennt man die mittlere Wahrscheinlichkeit der Daten oder Evidenz. Die Evidenz berechnet sich als Mittelwert der Likelihoods Ã¼ber alle Werte von \\(p\\). Die Aufgabe dieser GrÃ¶ÃŸe ist nur dafÃ¼r zu sorgen, dass insgesamt Werte zwischen 0 und 1 herauskommen. 4.5.10 Bayesâ€™ Theorem Bestandteile: Posteriori-Wahrscheinlichkeit: \\(Pr_{Post} := Pr(H|D)\\) Likelihood: \\(L := Pr(D|H)\\) Priori-Wahrscheinlichkeit: \\(Pr_{Priori} := Pr(H)\\) Evidenz: \\(E := Pr(D)\\) Bayesâ€™ Theorem gibt die \\(Pr_{Post}\\) an, wenn man die Gleichung mit der \\(Pr_{Priori}\\) und dem \\(L\\) fÃ¼ttert. Bayesâ€™ Theorem wird hÃ¤ufig verwendet, um die \\(Pr_{Post}\\) zu quantifizieren. Die \\(Pr_{Post}\\) ist proportional zu \\(L \\times Pr_{Priori}\\). 4.5.11 Posteriori als Produkt von Priori und Likelihood \\[\\text{Posteriori} = \\frac{\\text{Likelihood} \\times \\text{Priori}}{\\text{Evidenz}}\\] 4.6 Bayes berechnen mit R 4.6.1 Die Methode Gitter-AnnÃ¤herung7 Teile den Wertebereich des Parameter in ein â€œGitterâ€ auf, z.B. \\(0.1, 0.2, ..., 0.9, 1\\) (â€œGitterwerteâ€). Bestimme den Priori-Wert des Parameters fÃ¼r jeden Gitterwert. Berechne den Likelihood fÃ¼r Gitterwert. Berechne den unstandardisierten Posteriori-Wert fÃ¼r jeden Gitterwert (Produkt von Priori und Likelihood). Standardisiere den Posteriori-Wert durch teilen anhand der Summe alle unstand. Posteriori-Werte. 4.6.2 Gitterwerte in R berechnen d &lt;- tibble( # definiere das Gitter: p_Gitter = seq(from = 0, to = 1, length.out = 10), # bestimme den Priori-Wert: Priori = 1) %&gt;% mutate( # berechne Likelihood fÃ¼r jeden Gitterwert: Likelihood = dbinom(6, size = 9, prob = p_Gitter), # berechen unstand. Posteriori-Werte: unstd_Post = Likelihood * Priori, # berechne stand. Posteriori-Werte (summiert zu 1): Post = unstd_Post / sum(unstd_Post)) 4.6.3 Unsere Gitter-Daten p_Gitter Priori Likelihood unstd_Post Post 0.00 1 0.00 0.00 0.00 0.11 1 0.00 0.00 0.00 0.22 1 0.00 0.00 0.01 0.33 1 0.03 0.03 0.04 0.44 1 0.11 0.11 0.12 0.56 1 0.22 0.22 0.24 0.67 1 0.27 0.27 0.30 0.78 1 0.20 0.20 0.23 0.89 1 0.06 0.06 0.06 1.00 1 0.00 0.00 0.00 ğŸ‹ Was wohl mit Post passiert, wenn wir Priori Ã¤ndern? 4.6.4 \\(Pr_{Post}\\) zeigt, wie plausibel wir jeden Wert von \\(p\\) halten Mehr Gitterwerte glÃ¤tten die AnnÃ¤herung. 4.6.5 Je grÃ¶ÃŸer die Stichprobe (\\(N\\)), desto zuverlÃ¤ssiger wird unsere Berechnung Grau: Quadratische Anpassung; schwarz: wahre Verteilung 4.6.6 Zusammenfassung In unserem Modell haben wir Annahmen zu \\(Pr_{Priori}\\) und \\(L\\) getroffen. Auf dieser Basis hat der Golem sein Wissen geupdated zu \\(Pr_{Post}\\). Mit der Gitter-Methode haben wir viele Hypothesen (Parameterwerte) untersucht und jeweils die \\(Pr_{Post}\\) berechnet. Unser Modell bildet die kleine Welt ab; ob es in der groÃŸen Welt nÃ¼tzlich ist, steht auf einem anderen Blatt. ğŸ‹ Wenn Sie auf einen Prozentwert fÃ¼r \\(W\\) tippen mÃ¼ssten, welchen wÃ¼rden Sie nehmen, laut dem Modell (und gegeben der Daten)? 4.7 Literatur "],["die-post-verteilung.html", "Kapitel 5 Die Post-Verteilung 5.1 Lernsteuerung", " Kapitel 5 Die Post-Verteilung 5.1 Lernsteuerung 5.1.1 Lernziele erlÃ¤utern und mit R berechnen, wie man eine Posteriori-Verteilung erstellt erlÃ¤utern und mit R berechnen, wie man Stichproben aus der Posteriori-Verteilung zieht wesentliche Forschungsfragen, wie nach Intervallen und PunktschÃ¤tzwerten, anhand der Stichproben aus der Posteriori-Verteilung beantworten erlÃ¤utern, was eine Posteriori-PrÃ¤diktiv-Verteilung ist, und inwiefern Sie vor Ãœbergewissheit schÃ¼tzt eine ModellprÃ¼fung fÃ¼r das Beispiel aus dem Unterricht anhand der Posteriori-PrÃ¤diktiv-Verteilung durchfÃ¼hren 5.1.2 Literatur NA 5.1.3 Folien NA "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
