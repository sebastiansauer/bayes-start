[["index.html", "Bayes:Start Eine Einführung in die Bayes-Statistik Zu diesem Buch 0.1 tl;dr 0.2 Hinweise 0.3 Kudos", " Bayes:Start Eine Einführung in die Bayes-Statistik Sebastian Sauer Letzte Aktualisierung: 2022-07-07 21:38:41 Zu diesem Buch Bildquelle: Klara Schaumann, Lizenz: CC-BY 0.1 tl;dr Bayes-Start ist ein frei verfügbarer Einführungskurs in die Bayes-Statistik. Ziel ist, die grundlegende Logik von Bayes-Inferenz zu vermitteln und zu zeigen, wie man mit linearen Modellen viele typische Forschungsfragen beantworten kann. Ein wichtiger Baustein betrifft kausale Überlegungen, also die Frage, wie man kausale Forschungsfragen statistisch umsetzt und beurteilt. Als Software wird R verwendet R Core Team (2019) ; für Bayes-Modellierung wird das R-Paket rstanarm verwendet Goodrich et al. (2020) Inhaltlich wird zum Teil auf Richard McElreath’s hervorragenden Lehrbuch, Statistical Rethinking aufgebaut McElreath (2020). Außerdem stellt Gelman’s et al. neues, ebenfalls sehr gutes Lehrbuch Regression and other Stories Gelman, Hill, and Vehtari (2021) eine Grundlage für die Inhalte dieses Kurses dar. Beide Bücher sind als Lektüre zu empfehlen. 0.2 Hinweise Dieser Kurs ist lizensiert unter der MIT Lizenz. Das ist eine permissive Lizenz, die erlaubt, dass Sie diesen Kurs frei verwenden können. Sie haben (nur) die Verpflichtung, zu zitieren und auf die Lizenzart hinzuweisen. Installation von R und seinen Freunden Installation der Software für Bayes-Analysen Übergreifende Lernziele Mitarbeit oder Verbesserungsvorschläge: am besten als Github Issue einstellen. Der Quellcode zu den Folien findet sich hier. 0.3 Kudos Beim Schreiben dieses Kurses habe ich auf der Vorarbeit vieler Menschen aufgebaut. Viele Menschen haben mich unterstützt, großzügig und auf verschiedene Weise. Einige möchte ich herausgreifen, um Danke zu sagen: Kollegis wie Karsten Lübke, von denen ich viel gelernt habe. Richard McElreath für sein inspirierendes Lehrbuch. Alle Open-Source-Entwickler, die Projekte wie dieses überhaupt erst möglichen machen. 💌 References "],["hinweise-1.html", "Kapitel 1 Hinweise 1.1 Lernziele 1.2 Voraussetzungen 1.3 Lernhilfen 1.4 Modulzeitplan 1.5 Literatur 1.6 FAQ", " Kapitel 1 Hinweise knitr::opts_chunk$set(cache = FALSE) 1.1 Lernziele Nach diesem Kurs sollten Sie grundlegende Konzepte des statistischen Lernens verstehen und mit R anwenden können gängige Prognose-Algorithmen kennen, in Grundzügen verstehen und mit R anwenden können die Güte und Grenze von Prognosemodellen einschätzen können 1.2 Voraussetzungen Um von diesem Kurs am besten zu profitieren, sollten Sie folgendes Wissen mitbringen: grundlegende Kenntnisse im Umgang mit R, möglichst auch mit dem tidyverse grundlegende Kenntnisse der deskriptiven Statistik grundlegende Kenntnis der Regressionsanalyse 1.3 Lernhilfen 1.3.1 Software Installieren Sie R und seine Freunde. Installieren Sie die folgende R-Pakete: tidyverse tidymodels weitere Pakete werden im Unterricht bekannt gegeben (es schadet aber nichts, jetzt schon Pakete nach eigenem Ermessen zu installieren) R Syntax aus dem Unterricht findet sich im Github-Repo bzw. Ordner zum jeweiligen Semester. 1.3.2 Videos Playlist zu den Themen Auf dem YouTube-Kanal des Autors finden sich eine Reihe von Videos mit Bezug zum Inhalt dieses Buches. 1.3.3 Online-Zusammenarbeit Hier finden Sie einige Werkzeuge, die das Online-Zusammenarbeiten vereinfachen: Frag-Jetzt-Raum zum anonymen Fragen stellen während des Unterrichts. Der Keycode wird Ihnen vom Dozenten bereitgestellt. Padlet zum einfachen (und anonymen) Hochladen von Arbeitsergebnissen der Studentis im Unterricht. Wir nutzen es als eine Art Pinwand zum Sammeln von Arbeitsbeiträgen. Die Zugangsdaten stellt Ihnen der Dozent bereit. 1.4 Modulzeitplan 1.5 Literatur jklö 1.6 FAQ "],["inferenz.html", "Kapitel 2 Inferenz 2.1 Lernsteuerung 2.2 Was ist Inferenz? 2.3 Unsicherheit 2.4 Klassische vs. Bayes-Inferenz 2.5 Literatur", " Kapitel 2 Inferenz 2.1 Lernsteuerung 2.1.1 Vorbereitung Lesen Sie die Hinweise zum Modul. Installieren (oder Updaten) Sie die für dieses Modul angegeben Software. Lesen Sie die angegebene Literatur. 2.1.2 Lernziele die Definition von Inferenzstatistik sowie Beispiele für inferenzstatistische Fragestellungen nennen zentrale Begriffe der Inferenzstatistik nennen und in Grundzügen erklären den Nutzen von Inferenzstatistik nennen erläutern, in welchem Zusammenhang Ungewissheit zur Inferenzstatistik steht Unterschiede zwischen klassischer und Bayes-Inferenz benennen Vor- und Nachteile der klassischen vs. Bayes-Inferenz diskutieren Die grundlegende Herangehensweise zur Berechnung des p-Werts informell erklären können 2.1.3 Literatur ROS, Kap. 1 2.1.4 Hinweise Bitte beachten Sie die Hinweise zum Präsenzunterricht und der Streamingoption. Bitte stellen Sie sicher, dass Sie einen einsatzbereiten Computer haben und dass die angegebene Software (in aktueller Version) läuft. 2.2 Was ist Inferenz? 2.2.1 Deskriptiv- vs. Inferenzstatistik Deskriptivstastistik fasst Stichprobenmerkmale zu Kennzahlen (Statistiken) zusammen. Inferenzstatistik schließt von Statistiken auf Parameter (Kennzahlen von Grundgesamtheiten). 🏋 Schließen Sie die Augen und zeichnen Sie obiges Diagramm! 2.2.2 Wozu ist die Inferenstatistik gut? Inferenz bedeutet logisches Schließen; auf Basis von vorliegenden Wissen wird neues Wissen generiert. Inferenzstatistik ist ein Verfahren, das mathematische Modelle (oft aus der Stochastik) verwendet, um von einer bestimmten Datenlage, die eine Stichprobe einer Grundgesamtheit darstellt, allgemeine Schlüsse zu ziehen. 🏋️️ Heute Nacht vor dem Schlafen wiederholen Sie die Definition. Üben Sie jetzt schon mal. 2.2.3 Deskriptiv- und Inferenzstatistik gehen Hand in Hand 🏋️ Für jede Statistik (Kennzahl von Stichprobendaten) kann man die Methoden der Inferenzstatistik verwenden, z.B.: Für Statistiken (Stichprobe) verwendet man lateinische Buchstaben; für Parameter (Population) verwendet man griechische Buchstaben. 🏋️ Geben Sie die griechischen Buchstaben für typische Statistiken an! 2.2.4 Schätzen von Parametern einer Grundgesamtheit Meist begnügt man sich nicht mit Aussagen für eine Stichprobe, sondern will auf eine Grundgesamtheit verallgemeinern. Leider sind die Parameter einer Grundgesamtheit zumeist unbekannt, daher muss man sich mit Schätzungen begnügen. Schätzwerte werden mit einem “Dach” über dem Kennwert gekennzeichnet, z.B. 2.2.5 Beispiele für inferenzstatistische Fragestellungen Sie testen zwei Varianten Ihres Webshops (V1 und V2), die sich im Farbschema unterscheiden und ansonsten identisch sind: Hat das Farbschema einen Einfluss auf den Umsatz? Dazu vergleichen Sie den mittleren Umsatz pro Tag von V1 vs. V2, \\(\\bar{X}_{V1}\\) und \\(\\bar{X}_{V2}\\). Die Mittelwerte unterscheiden sich etwas, \\(\\bar{X}_{V1} &gt; \\bar{X}_{V2}\\) Sind diese Unterschiede “zufällig” oder “substanziell”? Gilt also \\(\\mu_{V1} &gt; \\mu_{V2}\\) oder gilt \\(\\mu_{V1} \\le \\mu_{V2}\\)? Wie groß ist die Wahrscheinlichkeit1 \\(Pr(\\mu_{V1} &gt; \\mu_{V2})\\)? 🏋️ Predictive Maintenance ist ein Anwendungsfeld inferenzstatistischer Modellierung. Lesen Sie dazu S. 3 dieses Berichts! 2.3 Unsicherheit 2.3.1 Inferenz beinhaltet Unsicherheit Inferenzstatistische Schlüsse sind mit Unsicherheit behaftet: Schließlich kennt man nur einen Teil (die Stichprobe) eines Ganzen (die Population), möchte aber vom Teil auf das Ganze schließen. Zur Bemessung der Unsicherheit bedient man sich der Wahrscheinlichkeitsrechnung (wo immer möglich). Die Wahrscheinlichkeitstheorie bzw. -rechnung wird auch als die Mathematik des Zufalls bezeichnet. Unter einem zufälligen Ereignis (random) verstehen wir ein Ereignis, das nicht (komplett) vorherzusehen ist, wie etwa die Augenzahl Ihres nächsten Würfelwurfs. Zufällig bedeutet nicht (zwangsläufig), dass das Ereignisse keine Ursachen besitzt. So gehorchen die Bewegungen eines Würfels den Gesetzen der Physik, nur sind uns diese oder die genauen Randbedingungen nicht (ausreichend) bekannt. 🏋 Welche physikalischen Randbedingungen wirken wohl auf einen Münzwurf ein? 2.3.2 Beispiele zur Quantifizierung von Ungewissheit Aussagen mit Unsicherheit können unterschiedlich präzise formuliert sein. Morgen regnet’s \\(\\Leftrightarrow\\) Morgen wird es hier mehr als 0 mm Niederschlag geben (\\(p=97\\%\\)). Methode \\(A\\) ist besser als Methode \\(B\\) \\(\\Leftrightarrow\\) Mit einer Wahrscheinlichkeit von 57% ist der Mittelwert für Methode \\(A\\) höher als für Methode \\(B\\). Die Maschine fällt demnächst aus \\(\\Leftrightarrow\\) Mit einer Wahrscheinlichkeit von 97% wird die Maschine in den nächsten 1-3 Tagen ausfallen, laut unserem Modell. Die Investition lohnt sich \\(\\Leftrightarrow\\) Die Investition hat einen Erwartungswert von 42 Euro; mit 90% Wahrscheinlichkeit wird der Gewinn zwischen -10000 und 100 Euro. 🏋 Geben Sie weitere Beispiele an! 2.3.3 Visualisierung von Punktschätzungen Rot markiert: Die Punktschätzung von mpg für hp=200. 🏋 Geben Sie ein vergleichbares Beispiel an! 2.3.4 Die Punktschätzung berücksichtigt nicht die Ungewissheit des Models Zwei Arten von Ungewissheit müssen wir (mindestens) in unseren Vorhersagen berücksichtigen: zur Lage der Regressionsgeraden (\\(\\beta_0\\), \\(\\beta_1\\)) zu Einflüssen, die unser Modell nicht kennt (\\(\\epsilon, \\sigma\\)) 2.3.4.1 Unsicherheit in \\(\\beta_0, \\beta1\\) 2.3.4.2 Unsicherheit durch \\(\\epsilon\\) (\\(\\sigma\\)) 2.3.5 Vorhersage-Intervall: berücksichtigt Ungewissheit in \\(\\beta_0, \\beta_1, \\epsilon\\) Das Vorhersage-Intervall berücksichtigt Ungewissheit in \\(\\beta_0, \\beta_1, \\epsilon\\) bei der Vorhersage von \\(\\hat{y_i}\\). 🏋 Interpretieren Sie den Ungewissheitskorridor! 2.4 Klassische vs. Bayes-Inferenz 2.4.1 Klassische Inferenz: Frequentismus Die Berücksichtigung von Vorwissen zum Sachgegenstand wird vom Frequentismus als subjektiv zurückgewiesen. Nur die Daten selber fliesen in die Ergebnisse ein Wahrscheinlichkeit wird über relative Häufigkeiten definiert. Es ist nicht möglich, die Wahrscheinlichkeit einer Hypothese anzugeben. Stattdessen wird angegeben, wie häufig eine vergleichbare Datenlage zu erwarten ist, wenn die Hypothese gilt und der Versuch sehr häufig wiederholt ist. Ein Großteil der Forschung (in den Sozialwissenschaften) verwendet diesen Ansatz. 2.4.2 Bayesianische Inferenz Vorwissen (Priori-Wissen) fließt explizit in die Analyse ein (zusammen mit den Daten). Wenn das Vorwissen gut ist, wird die Vorhersage genauer, ansonsten ungenauer. Die Wahl des Vorwissens muss explizit (kritisierbar) sein. In der Bayes-Inferenz sind Wahrscheinlichkeitsaussagen für Hypothesen möglich. Die Bayes-Inferenz erfordert mitunter viel Rechenzeit und ist daher erst in den letzten Jahren (für gängige Computer) komfortabel geworden. 2.4.3 Vergleich von Wahrscheinlichkeitsaussagen 2.4.3.1 Frequentismus zentrale Statistik: p-Wert “Wie wahrscheinlich ist der Wert der Teststatistik (oder noch extremere Werte), vorausgesetzt die Nullhypothese gilt und man wiederholt den Versuch unendlich oft (mit gleichen Bedingungen, aber zufällig verschieden und auf Basis unseres Modells)?” 2.4.3.2 Bayes-Statistik zentrale Statistik: Posteriori-Verteilung “Wie wahrscheinlich ist die Forschungshypothese, jetzt, nachdem wir die Daten kennen, auf Baiss unseres Modells?” 🏋 Recherchieren Sie eine Definition des p-Werts und lesen Sie sie genau. 2.4.4 Frequentist und Bayesianer Quelle 2.4.5 Der p-Wert ist wenig intuitiv from Imgflip Meme Generator 2.4.6 Beispiel zum Nutzen von Apriori-Wissen 1 Ein Betrunkener behauptet, er könne hellsehen. Er wirft eine Münze 10 Mal und sagt jedes Mal korrekt vorher, welche Seite oben landen wird. Die Wahrscheinlichkeit dieses Ergebnisses ist sehr gering (\\(2^{-10}\\)) unter der Hypothese, dass die Münze fair ist, dass Ergebnis also “zufällig” ist. Unser Vorwissen lässt uns allerdings trotzdem an der Hellsichtigkeit des Betrunkenen zweifeln, so dass die meisten von uns die Hypothese von der Zufälligkeit des Ergebnisses wohl nicht verwerfen. 2.4.7 Beispiel zum Nutzen von Apriori-Wissen 2 Eine Studie fand einen “großen Effekt” auf das Einkommen von Babies, eine Stunde pro Woche während zwei Jahren an einem psychosozialen Entwicklungsprogramm teilnahmen (im Vergleich zu einer Kontrollgruppe), \\(n=127\\). Nach 20 Jahren war das mittlere Einkommen der Experimentalgruppe um 42% höher (als in der Kontrollgruppe) mit einem Konfidenzintervall von [+2%,+98%]. Allerdings lässt uns unser Vorwissen vermuten, dass so ein Treatment das Einkommen nach 20 Jahren kaum verdoppeln lässt. Wir würden den Effekt lieber in einem konservativeren Intervall schätzen (enger um Null). 2.5 Literatur oft mit Pr oder p abgekürzt, für probability↩︎ "],["ungewissheit-quantifizieren.html", "Kapitel 3 Ungewissheit quantifizieren 3.1 Lernsteuerung 3.2 Was ist Wahrscheinlichkeit? 3.3 Zufallsexperiment 3.4 Additionsregel 3.5 Unabhängigkeit zweier Ereignisse 3.6 Multiplikationsregel für unabhängige Ereignisse 3.7 Hintergrundwissen 3.8 Bedingte Wahrscheinlichkeit", " Kapitel 3 Ungewissheit quantifizieren 3.1 Lernsteuerung 3.1.1 Lernziele Wahrscheinlichkeit definieren und relevante Begriffe anführen und in Grundzügen erklären einfache Fragen aus der Wahrscheinlichkeitstheorie berechnen 3.1.2 Literatur NA 3.2 Was ist Wahrscheinlichkeit? Die Wahrscheinlichkeit \\(p\\) quantifiziert Ungewissheit im Hinblick auf eine Aussage bzw. ein Ereignis \\(A\\), gegeben eines Hintergrundwissen \\(H\\). \\(p=0\\) heißt, wir halten die Aussage (das Ereignis) für falsch (unmöglich); \\(p=1\\) heißt, wir halten die Aussage (das Ereignis) für wahr (sicher). \\(0&lt;p&lt;1\\) heißt, wir sind (mehr oder weniger) unsicher bzgl. der Aussage bzw. ob das Ereignis zutrifft. \\(p(\\text{Kopf werfen mit einer fairen Münze}) = 1/2\\). \\(p(\\text{eine 6 würfeln mit einer fairen Würfel}) = 1/6\\). \\(p(\\text{Entweder ist heute Montag oder nicht}) = 1\\). \\(p(\\text{Berlin ist die Hauptstadt von Frankreich}) = 0\\). 🏋 Weitere Beispiele? 3.3 Zufallsexperiment Als Zufallsexperiment bezeichnen wir einen Vorgang, bei dem wir wissen, was alles passieren könnte, aber nicht sicher sind, was genau passiert. Die Menge der möglichen Ergebnisse nennt man Grundraum (Ergebnisraum) \\(\\Omega\\). Beim Würfelwurf: \\(\\Omega = \\{1, 2, 3, 4, 5, 6\\}\\) Jede Teilmenge \\(A \\subseteq \\Omega\\) nennt man ein Ereignis. Beim Würfelwurf: z.B. \\(A = \\{2, 4, 6\\}\\), eine gerade Zahl werfen. Ein Ereignis, das genau ein Element enthält, heißt Elementarereignis. Ein Ereignis, das alle Elementarereignisse aus \\(\\Omega\\) enthält, die nicht zum Ereignis \\(A\\) gehören, nennt man das Komplementärereignis (Komplement) \\(A^C\\) (auch: \\(\\overline{A}, \\neg A\\)). Beim Würfelwurf: Das Komplement von \\(A=\\{2,4,6\\}\\) ist \\(A^C=\\{1,3,5\\}\\), die ungeraden Zahlen. 🏋 Beschreiben Sie ein weiteres Zufallsexperiment! 3.4 Additionsregel Die Wahrscheinlichkeit, dass mindestens eines der beiden sich ausschließenden Ereignissen \\(A\\) und \\(B\\) der Fall ist, ist durch die Additionsregel gegeben: \\(Pr(\\text{A oder B}) = Pr(A \\cup B) = Pr(A) + Pr(B)\\) Beispiel: Wahrscheinlichkeit mit einem “fairen” Würfelwurf \\(X\\) eine 1 oder 2 zu werfen: \\(Pr(X=1 \\cup X=2) = Pr(X=1) + Pr(X=2) = 1/6 + 1/6 = 1/3\\) 🏋 Was ist \\(Pr(X &lt; 4)\\), \\(Pr(1 \\le X \\le 6)\\)? 3.5 Unabhängigkeit zweier Ereignisse Zwei Ereignisse sind (stochastisch) unabhängig, wenn Kenntnis des einen uns keine Information gibt, ob das andere der Fall ist. Ansonsten nennt man die beiden Ereignisse (stochastisch) abhängig oder zusammenhängend. Angenommen wir werfen zwei faire Münzen. Wir wissen, die erste Münze zeigt Kopf. Dieses Wissen gibt uns keine weitere Information, welche Seite bei der zweiten Münze oben liegt. Auf der anderen Seite sind Aktienkurs häufig voneinander abhängig. Weiß man, dass eine Aktie gestiegen ist, so ist es (häufig) wahrscheinlich, dass die andere auch gestiegen ist. Achtung: Stochastische (Un)abhängigkeit impliziert nicht kausale (Un)abhängigkeit. 3.5.1 Beispiele für abhängige und unabhängige Ereignisse \\(A\\) und \\(B\\) 3.5.2 Unabhängig Münzwurf 1 (A) und Münzwurf 2 (B), jeweils fair Meine Stimmung (A) und ob das heutige Datum eine Primzahl ist (B) Zwei Passanten getrennt zu ihrer Meinung zu einem politischen Thema befragen Die Körpergröße zweier zufällig gezogener Personen (A und B) 3.5.3 Abhängig Körpergröße zweier Geschwister (A und B) Lernleistung zwier Schüleris A und B der gleichen Klasse PS-Zahl (A) und Spritverbrauch (B) Augenzahl beim zweimaligen Wurf (A und B) eines gezinkten Würfels Geschlecht (A) und ob die Person Papst ist (B) 3.6 Multiplikationsregel für unabhängige Ereignisse Seien \\(A\\) und \\(B\\) zwei unabhängige Ereignisse, dann nennt man die gemeinsame Wahrscheinlichkeit \\(Pr(AB)\\), die Wahrscheinlichkeit, dass beide Ereignisse eintreten. Sie berechnet sich als Produkt der jeweiligen Wahrscheinlichkeiten von \\(A\\) und \\(B\\): \\[Pr(A \\text{ und } B ) = Pr(AB) = Pr(A \\cap B) = Pr(A) \\cdot Pr(B)\\] Wirft man zwei faire Münzen2, so ist die Wahrscheinlichkeit, dass beide Kopf zeigen: \\(Pr(KK) = Pr(K) \\cdot Pr(K) = 1/2 \\cdot 1/2 = 1/4\\). 🏋 Was ist \\(Pr(ZZ)\\)? Ist \\(Pr(ZK) = Pr(KZ)\\)? 3.6.1 Beispiele für die Multiplikationsregel unabhängiger Ereignisse Zwei Mal hintereinander eine 6 werfen (fairer Würfel): \\(Pr(6, 6) = Pr(6) \\cdot Pr(6) = 1/6 \\cdot 1/6 = 1/36\\). Beim Lotto wird erst die Zahl \\(42\\) und dann die Zahl \\(1\\) gezogen: \\(Pr(42,1)=1/49 \\cdot 1/48 \\approx 4.3\\times 10^{-4}\\). Bei der Klausur alle 10 Richtig-Falsch-Fragen richtig zu raten: \\(Pr(10r) = 1/2^{10} \\approx 0.001\\approx 1/1000\\) . Wenn man in der Disko 10 Personen anspricht, Korb-Wahrscheinlichkeit \\(p(K)=9/10\\) beträgt, wie hoch ist die Wahrscheinlichkeit nicht alleine nach hause zugehen? \\(Pr(\\neg0) = 1-0.9^{10} \\approx 0.65132\\). Ei Forschi führt 10 statistische Tests durch, jeweils mit 10% Gefahr, dass ein falsch-positives Ergebnis zustande kommt. Wie hoch ist die Wahrscheinlichkeit für mindestens 1 falsch-positives Ergebnis? \\(Pr(\\neg 0 FP) = 1 - 0.9^{10} \\approx 0.65\\) 3.6.2 Münzen werfen als Baum: Anzahl Kopf nach 2 Würfen Ereignis Pr 0K 1/2 * 1/2 = 1/4 1K 1/4 + 1/4 = 1/2 2K 1/2 * 1/2 = 1/4 🏋 Zeichnen Sie den Baum und berechnen Sie die Wahrscheinlichkeiten für eine gezinkte Münze mit \\(P(K) = 2/3\\). 3.6.3 Münzen werfen als Baum: Anzahl Kopf nach 3 Würfen Ereignis Pr 0K 1/2 * 1/2 * 1/2 = 1/8 1K 1/8 + 1/8 + 1/8 = 3/8 2K 3 * 1/8 = 3/8 3K 1/2 * 1/2 * 1/2 = 1/8 3.7 Hintergrundwissen 3.7.1 Wahrscheinlichkeit ist abhängig vom Hintergrundwissen (\\(H\\)) \\(Pr(A|H)\\): Die Wahrscheinlichkeit von \\(A\\), gegeben \\(H\\). A: “Sokrates ist sterblich.”; H: “Alle Menschen sind sterblich und Sokrates ist ein Mensch.” \\(\\implies Pr(A|H) = 1\\). A: “Die Münze zeigt Kopf”; H: “Wir haben keinen Grund anzunehmen, dass eine der beiden Seiten häufiger oben liegt oder das sonst etwas passiert.” \\(\\implies Pr(A|H)=1/2\\). A: “Schorsch, das rosa Einhort, mag Bier.”; H: “50% der rosa Einhörner mögen Bier.” \\(\\implies Pr(A|H) = 1/2\\). Die Wahrscheinlichkeit eine 6 zu würfeln (A), gegeben dem Hintergrundwissen (H), dass der Würfel “fair” ist, d.h. wir kein Wissen haben, dass eine Augenzahl häufiger auftritt, ist \\(1/6\\). 3.7.2 Hintergrundwissen ist subjektiv Ich habe gerade einen Stift in meiner Hosentasche (links oder rechts). Wie groß ist die Wahrscheinlichkeit, dass der Stift in meiner linken Tasche ist (und nicht in der rechten)? Bezogen auf Ihr Hintergrundwissen gilt: \\(Pr(\\text{A=&quot;Stift links&quot;|H=&quot;kein besonderes Wissen zu der Frage&quot;}) = 1/2\\). Bezogen auf mein Hintergrundwissen gilt: \\(Pr(\\text{A=&quot;Stift links&quot;|H=&quot;Der Stift ist links&quot;}) = 1\\). Briggs (2016) 🏋 Geben Sie ein weiteres Beispiel für die Subjektivität von Hintergrundwissen an! Formalisieren Sie es wie oben gezeigt. 3.8 Bedingte Wahrscheinlichkeit Wie groß ist die Wahrscheinlichkeit: die Klausur zu bestehen, wenn man gelernt hat? von schlechter Laune, gegeben es ist Montag? schwer an Corona zu erkranken, unter der Bedingung, man ist geimpft? \\(Pr(A|H)\\) ist die Wahrscheinlichkeit, dass \\(A\\) eintritt, gegeben bzw. unter der Bedingung, dass \\(H\\) eingetreten ist. Formel der bedingten Wahrscheinlichkeit: \\[Pr(B|l) = \\frac{Pr(B \\cap l)}{Pr(l)}\\] 3.8.1 Kontingenztabelle zur Berechnung von Wahrscheinlichkeiten Beispiel aus den Klausurergebnissen bei Prof. Süß: . bestanden (B) nicht (¬B) SUMME hat gelernt (l) 36 6 42 nicht (¬l) 12 24 36 SUMME 48 30 78 Randwahrscheinlichkeit: \\(Pr(B) = 48/78 \\approx 0.61 \\quad Pr(l) = 42/78 \\approx 0.54\\) Bedingte Wahrscheinlichkeit: \\(Pr(B|l) = 36/42 \\approx \\frac{0.46}{0.54} \\approx 0.86 \\quad Pr(l|B) = 36/48 = 0.75\\) Gemeinsame Wahrscheinlichkeit: \\(Pr(B \\cap l) = Pr(l \\cap B) = Pr(Bl) = Pr(lB) = 36/78 \\approx 0.46\\) 3.8.2 Visualierung von gemeinsamer und bedingter Wahrscheinlichkeit \\(Pr(AB) = Pr(A) \\cdot Pr(B) = 50\\% \\cdot 50\\% = 25\\%\\) \\(Pr(A|B) = Pr(A,B) / Pr(B) = 25\\% / 50\\% = 50\\%\\) 3.8.3 Visualierung von (un)abhängigen Ereignissen Ändert sich die Wahrscheinlichkeit eines Ereignisses, wenn man es auf ein anderes bedingt, so sind beide Ereignisse abhängig, ansonsten unabhängig. 3.8.3.1 Abhängig \\(P(A|B) \\ne Pr(A) \\ne Pr(A|\\neg B)\\) Überleben auf der Titanic ist abhängig von der Passagierklasse. 3.8.3.2 Unabhängig \\(P(A|B) = Pr(A) = Pr(A|\\neg B)\\) Überleben auf der Titanic ist unabhängig vom Ereignis Alter ist eine Primzahl. 3.8.4 Beispiel zur Visualisierung zweier abhängiger Ereignisse Sind die Ereignisse Tod durch Covid bzw. Impfquote (\\(A\\)) und Land3 (\\(B\\)) voneinander abhängig? Ja, da in beiden Diagrammen gilt: \\(P(A|B) \\ne Pr(A) \\ne Pr(A|\\neg B)\\). Daten von Our World in Data Hannah Ritchie and Roser (2020) References "],["bayes.html", "Kapitel 4 Bayes 4.1 Lernsteuerung 4.2 Kleine Welt, große Welt 4.3 Bayes-Statistik als Zählen 4.4 Ein erstes Modell 4.5 Der datengenierende Prozess: Wie entstanden die Daten? 4.6 Bayes berechnen mit R 4.7 Literatur", " Kapitel 4 Bayes 4.1 Lernsteuerung 4.1.1 Lernziele Wahrscheinlichkeit definieren und relevante Begriffe anführen und in Grundzügen erklären einfache Fragen aus der Wahrscheinlichkeitstheorie berechnen 4.1.2 Literatur NA 4.2 Kleine Welt, große Welt 4.2.1 Behaims Globus, Kolumbus glücklicher Fehler Quelle 4.2.2 Kleine Welt vs. große Welt 4.2.2.1 Kleine Welt Die Welt, wie sie der Golem sieht entspricht dem Modell (zwangläufig) 4.2.2.2 Große Welt Die Welt, wie sie in Wirklichkeit ist entspricht nicht (zwangsläufig) dem Modell Behaims Globus ist nicht gleich der Erde. Die kleine Welt ist nicht die große Welt. Was in der kleinen Welt funktioniert, muss nicht in der großen Welt funktionieren. Modelle zeigen immer nur die kleine Welt: Vorsicht vor schnellen Schlüssen und vermeintlicher Gewissheit. 🏋 Nennen Sie ein Beispiel, in dem ein Modell nicht (exakt) der Wirklichkeit entspricht! 4.2.3 So denkt unser Bayes-Golem 🏋 Bayes-Inferenz ähnelt dem Lernen von Menschen. Geben Sie ein Beispiel von Lernen bei Menschen, das oben dargestelltem Prozess ähnelt! 4.3 Bayes-Statistik als Zählen 4.3.1 Murmeln im Säckchen Sie haben ein Säckchen mit vier Murmeln darin. Sie wissen nicht, welche Farben die Murmeln haben. Murmeln gibt’s in zwei Farben: weiß (W) oder blau (B). Es gibt daher fünf Hypothesen zur Farbe der Murmeln im Säckchen: [WWWW], [BWWW], [BBWW], [BBBW], [BBBB.] Unser Ziel ist, die Wahrscheinlichkeiten der Hypothesen nach Ziehen von Murmeln zu bestimmen. 4.3.2 Unsere Daten Wir ziehen eine Murmel, merken uns die Farbe und legen sie zurück. Das wiederholen wir noch zwei Mal (Ziehen mit Zurücklegen). Wir erhalten: BWB. Voilà: unsere Daten. (Kurz 2021) 🏋 Wie groß ist die Stichprobe (\\(N\\))? Ist die Wahrscheinlichkeit für B in jedem Zug gleich? 4.3.3 Zugmöglichkeiten laut Hypothese [BWWW], 1. Zug Wenn Hypothese [BWWW] der Fall sein sollte, dann können wir im ersten Zug entweder die eine blaue Murmel erwischen oder eine der drei weißen. Nachdem wir die Murmel gezogen haben (und die Farbe gemerkt haben), legen wir sie wieder ins Säckchen: Ziehen mit Zurücklegen. 🏋 Wie viele Elementarereignisse hat dieses Zufallsexperiment? Sind alle gleich wahrscheinlich? 4.3.4 Zugmöglichkeiten laut Hypothese [BWWW], 1. und 2. Zug Wenn Hypothese [BWWW] der Fall sein sollte, dann haben wir im zweiten Zug natürlich die gleichen Möglichkeiten wie im ersten. Zug 1 und Zug 2 zusammen genommen gibt es \\(16=4\\cdot4=4^2\\) Kombinationen an gezogenen Murmeln: 🏋 Ist jedes Elementarereignis (z.B. BB, BW,…) gleich wahrscheinlich? 4.3.5 Zugmöglichkeiten laut Hypothese [BWWW], 1.-3. Zug Zug 1, Zug 2 und Zug 3 zusammen genommen, gibt es dann \\(4\\cdot4\\cdot4=4^3=64\\) Kombinationen, drei Murmeln zu ziehen. 🏋 Wie wahrscheinlich ist ein bestimmtes dieser 64 Ereignisse (unter der Annahme gleicher Wahrscheinlichkeit)? 4.3.6 Welche Züge sind logisch möglich? Einige Kombinationen (“Pfade”) der Hypothese [BWWW] lassen sich nicht mit unseren Daten (BWB) vereinbaren. Z.B. alle Kombinationen die mit W beginnen, sind nicht mit unseren Daten zu vereinbaren. Nur 3 der 64 “Pfade” (Kombinationen), die Hypothese [BWWW] vorgibt, sind mit unseren Daten logisch zu vereinbaren. 4.3.7 Kombinationen für Hypothesen Hypothese Häufigkeit BWB [W W W W] 0 * 4 * 0 = 0 [B W W W] 1 * 3 * 1 = 3 [B B W W] 2 * 2 * 2 = 8 [B B B W] 3 * 1 * 3 = 9 [B B B B] 4 * 0 * 4 = 0 Die Häufigkeiten der Kombinationen (Pfade) ist proportional zur Plausibilität einer Hypothese: Je mehr Pfade laut Hypothese, desto wahrscheinlicher die Hypothese (unter sonst gleichen Bedingungen). Zusätzlich müssten wir noch beachten, ob bestimmte Hypothesen per se bzw. a priori wahrscheinlicher sind. So könnten blaue Murmeln selten sein. Gehen wir der Einfachheit halber zunächst davon aus, dass alle Hypothesen apriori gleich wahrscheinlich sind. 4.3.8 Pfadbaum für die Hypothesen [BWWW], [BBWW], [BBBW] 4.3.9 Wir ziehen einer vierte Murmel: B Gehen wir zunächst davon aus, dass alle Hypothesen apriori gleich wahrscheinlich sind. Wir ziehen wieder eine Murmel. Sie ist blau (B)! Jetzt könnten wir den Pfadbaum für vier (statt drei) Züge aufmalen. Oder wir machen ein Update: Wir aktualisieren die bisherigen Kombinationshäufigkeiten um die neuen Daten. Die alten Daten dienen dabei als Priori-Informationen für die neuen Daten. 4.3.10 Priori-Information nutzen Mit den Daten BWBB ist die Hypothese [BBBW] am wahrscheinlichsten: Hyp PB HA HN [W W W W] 0 0 0 * 0 = 0 [B W W W] 1 3 1 * 3 = 3 [B B W W] 2 8 2 * 8 = 16 [B B B W] 3 9 3 * 9 = 27 [B B B B] 4 0 4 * 0 = 0 Hyp: Hypothese PB: Anzahl von Pfaden für B HA: alte (bisherige) Häufigkeiten HN: neue (geupdatete) Häufigkeiten 4.3.11 Murmelfabrik streikt: Blaue Murmeln jetzt sehr selten! Berücksichtigen wir jetzt die Information, dass apriori (bevor wir die Daten gesehen haben), einige Hypothesen wahrscheinlicher (plausibler) sind als andere. Hier ist die Hypothese [BBWW] am wahrscheinlichsten: Hyp HA HF HN [W W W W] 0 0 0 * 0 = 0 [B W W W] 3 3 3 * 3 = 9 [B B W W] 16 2 16 * 2 = 32 [B B B W] 27 1 27 * 1 = 27 [B B B B] 0 0 0 * 0 = 0 HF: Häufigkeit des Säckchentyps laut Fabrik. 4.3.12 Zählen mit großen Zahlen nervt Malen Sie mal den Pfadbaum für 10 Züge … Eine Umrechnung der Häufigkeiten in Anteile macht das Rechnen einfacher. Dazu definieren wir die geupdatete Plausibilität einer Hypothese nach Kenntnis der Daten: \\[\\text{Plausibilität von [BWWW] nach Kenntnis von BWB}\\] \\[\\propto\\] \\[\\text{Anzahl möglicher Pfade bei [BWWW] für BWB}\\] \\[\\times\\] \\[\\text{Priori-Plausibilität von [BWWW]}\\] \\(\\propto\\): proportional zu 4.3.13 Plausibilität berechnen Sei \\(p\\) der Anteil blauer Murmeln. Bei Hypothese [BWWW] gilt, dann ist \\(p=1/4 = 0.25\\). Sei \\(D_{neu} =\\) BWB, die Daten: \\[\\text{Plausibilität von }p\\text{ nach Kenntnis von }D_{neu}\\] \\[\\propto\\] \\[\\text{Anzahl Pfade von }p\\text{ für }D_{neu}\\] \\[\\times\\] \\[\\text{Priori-Plausibilität von }p\\] Für jeden Wert von \\(p\\) beurteilen wir dessen Plausibilität als umso höher, je mehr Pfade durch den Pfadbaum führen und je höher die Plausibilität des Werts von \\(p\\) von vornherein ist. 4.3.14 Von Plausibilität zur Wahrscheinlichkeit Teilen wir die Anzahl Pfade einer Hypothese durch die Anzahl aller Pfade (aller Hypothesen), so bekommen wir einen Anteil. Damit haben wir eine Wahrscheinlichkeit: \\[\\text{Pl von }p\\text{ mit Daten }D_{neu} =\\] \\[\\frac{\\text{Anzahl Pfade von }p\\text{ für }D_{neu}\\times \\text{Prior-Pl von }p}{\\text{Summe aller Pfade}}\\] Pl: Plausibilität 🏋 Was muss passieren, dass der Bruch gleich Null ist? 4.3.15 Plausibilität pro Hypothese source(&quot;R-Code/tab24.R&quot;) Hyp p AP Pl [W W W W] 0.00 0 0.00 [B W W W] 0.25 3 0.15 [B B W W] 0.50 8 0.40 [B B B W] 0.75 9 0.45 [B B B B] 1.00 0 0.00 p: Anteil blauer Murmeln (Priori-Wissen) AP: Anzahl von möglichen Pfaden; Pl: Plausibilität AP &lt;- c(0, 3, 8, 9, 0) Pl &lt;- AP / sum(AP) Pl ## [1] 0.00 0.15 0.40 0.45 0.00 4.3.16 Fachbegriffe Kennwerte laut einer Hypothese, wie den Anteil blauer Murmeln \\(p\\) bezeichnet man als Parameter. Den Anteil gültiger Pfade pro Hypothese (bzw. pro Wert von \\(p\\)) bezeichnet man als Likelihood. Die Priori-Plausibilität nennt man Priori-Wahrscheinlichkeit. Die neue, geupdatete Plausibilität für einen bestimmten Wert von \\(p\\) nennt man Posteriori-Wahrscheinlichkeit. 🏋 Erklären Sie die Begriffe dem nächsten Menschen, den Sie treffen! 4.3.17 Zusammenfassung Schritt: Unser Vorab-Wissen zur Wahrscheinlichkeit jeder Hypothese wird mit dem Begriff Priori-Verteilung gefasst. Schritt: Wir zählen den Anteil gültiger Pfade für jede Hypothese; d.h. wir berechnen den Likelihood jeder Hypothese. Schritt: Mit den Likelihoods updaten wir unsere Priori-Verteilung. Die Wahrscheinlichkeit jeder Hypothese verändert sich entsprechend der Daten. Es resultiert die Posteriori-Verteilung. 4.4 Ein erstes Modell 4.4.1 Welcher Anteil der Erdoberfläche ist mit Wasser bedeckt? knitr::include_graphics(&quot;img/earth.png&quot;) Quelle CC 4.0 BY-NC Sie werden einen Globus-Ball in die Luft und fangen in wieder auf. Sie notieren dann, ob die Stelle unter Ihrem Zeigefinger Wasser zeigt (W) oder Land (L). Den Versuch wiederholen Sie 9 Mal. \\[W \\quad L \\quad W \\quad W \\quad W \\quad L \\quad W \\quad L \\quad W\\] 🏋 Besorgen Sie sich einen Globus (zur Not eine Münze) und stellen Sie den Versuch nach! 4.5 Der datengenierende Prozess: Wie entstanden die Daten? Der wahre Anteil von Wasser der Erdoberfläche ist \\(p\\). Ein Wurf des Globusballes hat die Wahrscheinlichkeit \\(p\\), eine \\(W\\)-Beobachtung zu erzeugen. Die Würfe des Globusballes sind unabhängig voneinander. Wir haben kein Vorwissen über \\(p\\); jeder Wert ist uns gleich wahrscheinlich. 🏋 Welche Annahmen würden Sie ändern? Welche könnte man wegnehmen? Welche hinzufügen? Was wären die Konsequenzen? 4.5.1 Wissen updaten: Wir füttern Daten in das Modell 4.5.2 Erinnern wir uns an das Urnen-Beispiel Für jede Hypothese haben wir ein Vorab-Wissen, das die jeweilige Plausibilität der Hypothese angibt: Priori-Verteilung. Für jede Hypothese (d.h. jeden Parameterwert \\(p\\)) möchten wir den Anteil (die Wahrscheinlichkeit) gültiger Kombinationen wissen. Das gibt uns den Likelihood. Dann gewichten wir den Likelihood mit dem Vorabwissen, so dass wir die Posteriori-Verteilung4 bekommen. 4.5.3 Die Binomialverteilung Wir nehmen an, dass die Daten unabhängig voneinander entstehen und sich der Parameterwert nicht zwischenzeitlich ändert. Dann kann man die Wahrscheinlichkeit (\\(Pr\\)), \\(W\\) mal Wasser und \\(L\\) mal Land zu beobachten, wenn die Wahrscheinlichkeit für Wasser \\(p\\) beträgt, mit der Binomialverteilung berechnen. Die Binomialverteilung zeigt die Verteilung der Häufigkeit (Wahrscheinlichkeit) der Ereignisse (z.B. 2 Mal Kopf) beim wiederholten Münzwurf (und allen vergleichbaren Zufallsexperimenten)5. \\[Pr(W,L|p) = \\frac{(W+L)!}{W!L!}p^W(1-p)^L\\] 4.5.4 Binomialverteilung mit R Was ist der Anteil der gültigen Pfade (Wahrscheinlichkeit), um 6 mal \\(W\\) bei \\(N=W+L=9\\) Würfen zu bekommen, wenn wir von \\(p=1/2\\) ausgehen? dbinom(x = 6, size = 9, prob = 1/2) ## [1] 0.1640625 Was ist die Wahrscheinlichkeit für \\(W=9\\) bei \\(N=9\\) und \\(p=1/2\\)? dbinom(x = 9, size = 9, prob = 1/2) ## [1] 0.001953125 4.5.5 Beispiele zur Berechnung einer binomial verteilten Wahrscheinlichkeit Ei Professori stellt einen Klausur mit 20 Richtig-Falsch-Fragen. Wie groß ist die Wahrscheinlichkeit, durch bloßes Münze werfen genau 15 Fragen richtig zu raten?6 dbinom(x = 15, size = 20, prob = .5) ## [1] 0.01478577 Was ist die Wahrscheinlichkeit bei 3 Münzwürfen (genau) 3 Treffer (Kopf) zu erzielen? dbinom(3, 3, 1/2) ## [1] 0.125 4.5.6 Unser Modell ist geboren Wir fassen das Globusmodell so zusammen: \\[W \\sim \\text{Bin}(N,p),\\] Lies: “W ist binomial verteilt mit den Parametern \\(N\\) und \\(p\\)”. \\(N\\) gibt die Anzahl der Globuswürfe an: \\(N=W+L\\). Unser Vorab-Wissen zu \\(p\\) sei, dass uns alle Werte gleich plausibel erscheinen (“uniform”): \\[p \\sim \\text{Unif}(0,1).\\] Lies: “\\(p\\) ist gleich (uniform) verteilt mit der Untergrenze 0 und der Obergrenze 1”. 4.5.7 So sehen die Verteilungen aus 4.5.7.1 Binomialverteilung \\(N=9, p = 1/2\\) 4.5.7.2 Gleichverteilung \\(Min = 0, Max = 1\\) 🏋 Was fällt Ihnen bei der Binomialverteilung auf? Ist sie symmetrisch? Verändert sich die Wahrscheinlichkeit linear? Was fällt Ihnen bei der Gleichverteilung auf? 4.5.8 Herleitung Bayes’ Theorem 1/2: Gemeinsame Wahrscheinlichkeit Die Wahrscheinlichkeit für Regen und kalt ist gleich der Wahrscheinlihckeit von Regen, gegeben kalt mal der Wahrscheinlicht von kalt. Entsprechend gilt: Die Wahrscheinlichkeit von \\(W\\), \\(L\\) und \\(p\\) ist das Produkt von \\(Pr(W,L|p)\\) und der Prior-Wahrscheinlichkeit \\(Pr(p)\\): \\[Pr(W,L,p) = Pr(W,L|p) \\cdot Pr(p)\\] Genauso gilt: Die Wahrscheinlichkeit von Regen und kalt ist gleich der Wahrscheinlichkeit kalt, wenn’s regnet mal der Wahrscheinlichkeit von Regen: \\[Pr(W,L,p) = Pr(p|W,L) \\cdot Pr(W, L)\\] 4.5.9 Herleitung Bayes’ Theorem 2/2: Posteriori-Wahrscheinlichkeit Wir setzen die letzten beiden Gleichungen gleich: \\[Pr(W,L|p) \\cdot Pr(p) = Pr(p|W,L) \\cdot (W,L)\\] Und lösen auf nach der Posteriori-Wahrscheinlichkeit, \\(Pr(p|W,L)\\): \\[Pr(p|W,L) = \\frac{Pr(W,L|p) Pr(p)}{Pr(W,L)}\\] \\(Pr(W,L)\\) nennt man die mittlere Wahrscheinlichkeit der Daten oder Evidenz. Die Evidenz berechnet sich als Mittelwert der Likelihoods über alle Werte von \\(p\\). Die Aufgabe dieser Größe ist nur dafür zu sorgen, dass insgesamt Werte zwischen 0 und 1 herauskommen. 4.5.10 Bayes’ Theorem Bestandteile: Posteriori-Wahrscheinlichkeit: \\(Pr_{Post} := Pr(H|D)\\) Likelihood: \\(L := Pr(D|H)\\) Priori-Wahrscheinlichkeit: \\(Pr_{Priori} := Pr(H)\\) Evidenz: \\(E := Pr(D)\\) Bayes’ Theorem gibt die \\(Pr_{Post}\\) an, wenn man die Gleichung mit der \\(Pr_{Priori}\\) und dem \\(L\\) füttert. Bayes’ Theorem wird häufig verwendet, um die \\(Pr_{Post}\\) zu quantifizieren. Die \\(Pr_{Post}\\) ist proportional zu \\(L \\times Pr_{Priori}\\). 4.5.11 Posteriori als Produkt von Priori und Likelihood \\[\\text{Posteriori} = \\frac{\\text{Likelihood} \\times \\text{Priori}}{\\text{Evidenz}}\\] 4.6 Bayes berechnen mit R 4.6.1 Die Methode Gitter-Annäherung7 Teile den Wertebereich des Parameter in ein “Gitter” auf, z.B. \\(0.1, 0.2, ..., 0.9, 1\\) (“Gitterwerte”). Bestimme den Priori-Wert des Parameters für jeden Gitterwert. Berechne den Likelihood für Gitterwert. Berechne den unstandardisierten Posteriori-Wert für jeden Gitterwert (Produkt von Priori und Likelihood). Standardisiere den Posteriori-Wert durch teilen anhand der Summe alle unstand. Posteriori-Werte. 4.6.2 Gitterwerte in R berechnen d &lt;- tibble( # definiere das Gitter: p_Gitter = seq(from = 0, to = 1, length.out = 10), # bestimme den Priori-Wert: Priori = 1) %&gt;% mutate( # berechne Likelihood für jeden Gitterwert: Likelihood = dbinom(6, size = 9, prob = p_Gitter), # berechen unstand. Posteriori-Werte: unstd_Post = Likelihood * Priori, # berechne stand. Posteriori-Werte (summiert zu 1): Post = unstd_Post / sum(unstd_Post)) 4.6.3 Unsere Gitter-Daten p_Gitter Priori Likelihood unstd_Post Post 0.00 1 0.00 0.00 0.00 0.11 1 0.00 0.00 0.00 0.22 1 0.00 0.00 0.01 0.33 1 0.03 0.03 0.04 0.44 1 0.11 0.11 0.12 0.56 1 0.22 0.22 0.24 0.67 1 0.27 0.27 0.30 0.78 1 0.20 0.20 0.23 0.89 1 0.06 0.06 0.06 1.00 1 0.00 0.00 0.00 🏋 Was wohl mit Post passiert, wenn wir Priori ändern? 4.6.4 \\(Pr_{Post}\\) zeigt, wie plausibel wir jeden Wert von \\(p\\) halten Mehr Gitterwerte glätten die Annäherung. 4.6.5 Je größer die Stichprobe (\\(N\\)), desto zuverlässiger wird unsere Berechnung Grau: Quadratische Anpassung; schwarz: wahre Verteilung 4.6.6 Zusammenfassung In unserem Modell haben wir Annahmen zu \\(Pr_{Priori}\\) und \\(L\\) getroffen. Auf dieser Basis hat der Golem sein Wissen geupdated zu \\(Pr_{Post}\\). Mit der Gitter-Methode haben wir viele Hypothesen (Parameterwerte) untersucht und jeweils die \\(Pr_{Post}\\) berechnet. Unser Modell bildet die kleine Welt ab; ob es in der großen Welt nützlich ist, steht auf einem anderen Blatt. 🏋 Wenn Sie auf einen Prozentwert für \\(W\\) tippen müssten, welchen würden Sie nehmen, laut dem Modell (und gegeben der Daten)? 4.7 Literatur "],["die-post-verteilung.html", "Kapitel 5 Die Post-Verteilung 5.1 Lernsteuerung", " Kapitel 5 Die Post-Verteilung 5.1 Lernsteuerung 5.1.1 Lernziele erläutern und mit R berechnen, wie man eine Posteriori-Verteilung erstellt erläutern und mit R berechnen, wie man Stichproben aus der Posteriori-Verteilung zieht wesentliche Forschungsfragen, wie nach Intervallen und Punktschätzwerten, anhand der Stichproben aus der Posteriori-Verteilung beantworten erläutern, was eine Posteriori-Prädiktiv-Verteilung ist, und inwiefern Sie vor Übergewissheit schützt eine Modellprüfung für das Beispiel aus dem Unterricht anhand der Posteriori-Prädiktiv-Verteilung durchführen 5.1.2 Literatur NA 5.1.3 Folien NA "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
